{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "401ebc51-0994-497e-8a29-fdd9a117e5c7",
   "metadata": {},
   "source": [
    "# Importing Data and reading Xlsx File"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f729383-278c-4451-a8ae-ab36762c87f3",
   "metadata": {},
   "source": [
    "Main Objectives\n",
    "\n",
    "- 1. **Predicting Incurred Amount (Regression)**\n",
    "   \n",
    "Estimate the monetary cost of a claim — the total liability the insurer expects to pay (includes medical bills, repairs, settlements, etc.).\n",
    "\n",
    "Why it matters to the business:\n",
    "\n",
    "- Accurate Reserving\n",
    "\n",
    "- Insurers must set aside money (reserves) to cover expected payouts. Overestimating ties up cash unnecessarily; underestimating causes solvency risks.\n",
    "\n",
    "- Financial Forecasting\n",
    "\n",
    "- Enables CFOs and actuaries to project financial exposure, capital needs, and reinsurance planning.\n",
    "\n",
    "- Fraud & Anomaly Detection\n",
    "\n",
    "- Large differences between predicted vs. actual incurred amounts can flag outlier claims or suspicious behavior.\n",
    "\n",
    "- Operational Planning\n",
    "\n",
    "H- elps managers allocate experienced adjusters and legal resources to high-value claims early.\n",
    "\n",
    "- 2. **Classifying High-Cost Claims (Binary Classification: 0/1)**\n",
    "\n",
    "Identify whether a claim will become \"high-cost\" (e.g., in top 1–5% of cost distribution) as early as possible.\n",
    "\n",
    "Why it matters to the business:\n",
    "\n",
    "- Proactive Intervention\n",
    "\n",
    "- Early detection of costly claims allows escalation to senior adjusters, use of special investigators, or negotiation strategies.\n",
    "\n",
    "- Cost Containment\n",
    "\n",
    "- Timely action can prevent claims from ballooning (e.g., managing medical treatments, dispute resolution, rental periods).\n",
    "\n",
    "- Customer Strategy\n",
    "\n",
    "- Flagging high-cost claims helps tailor communication, retain key customers, or make legal preparations early.\n",
    "\n",
    "- Portfolio Risk Profiling\n",
    "\n",
    "- Helps the underwriting team understand which customer or product segments produce high-cost claims and adjust policies or pricing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43a2cb18-45ef-4179-b44c-a8f96ae2f092",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "data_file = pd.ExcelFile('Data_Scientist_Interview_Task.xlsx')\n",
    "\n",
    "print(data_file.sheet_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a4bfbc2-3da5-4a4e-9d93-f6f846245cf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel(data_file, sheet_name='Data')\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1df8dad6-9068-4b4c-bf91-15ebffe433ff",
   "metadata": {},
   "source": [
    "# Data Cleansing and Issue Identification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d489f09-eab4-4f5c-a973-1a38efa65585",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90281ad7-d467-4f91-a137-335f488d5190",
   "metadata": {},
   "source": [
    "* weather conditions is the only feature which has null values which need to be transformed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a49fee59-292b-481d-9807-577fabf4eddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Weather_conditions'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efbde547-132e-4687-909a-bca462921574",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ac691ef-e0c2-435c-831f-42acd52d124b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['date_of_loss'] = pd.to_datetime(df['date_of_loss'], errors='coerce')\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2ba6803-9f9a-410d-875c-87b62e4f493b",
   "metadata": {},
   "source": [
    "# Fill Nan values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd5dcc41-7205-48ab-8996-73beb07313e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14, 6))\n",
    "sns.heatmap(df.isnull(), cbar=False, yticklabels=False, cmap='viridis')\n",
    "plt.title(\"Missing Values Heatmap\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44abadaa-1ba8-42b0-9a0f-db91f9df5458",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_cols = df.select_dtypes(include='object').columns\n",
    "df[cat_cols] = df[cat_cols].fillna('Missing')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0836f33f-c31b-4e0f-9038-2562fd43d206",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_cols = df.select_dtypes(include=np.number).columns\n",
    "\n",
    "df[num_cols] = df[num_cols].fillna(0)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01bfb603-ab99-4044-b897-161bf0380b20",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d82fced-c2ad-48d2-8965-01797aec740c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['TP_injury_whiplash'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d4df7c4-eba4-4b4b-93cd-65ca00cae1eb",
   "metadata": {},
   "source": [
    "# Outlier Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d66a780f-d24d-49ea-a597-110ec6e18bd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "important_numerical_cols = [\n",
    "    'Incurred', 'Capped Incurred', 'Notification_period', 'Inception_to_loss',\n",
    "    'TP_type_driver', 'TP_type_pass_front', 'TP_type_pass_back',\n",
    "    'TP_type_pedestrian', 'TP_type_other',\n",
    "    'TP_injury_whiplash', 'TP_injury_traumatic', 'TP_injury_fatality'\n",
    "]\n",
    "\n",
    "def detect_outliers(df, columns):\n",
    "    outlier_summary = {}\n",
    "    for col in columns:\n",
    "        Q1 = df[col].quantile(0.25)\n",
    "        Q3 = df[col].quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "        lower = Q1 - 1.5 * IQR\n",
    "        upper = Q3 + 1.5 * IQR\n",
    "        outliers = df[(df[col] < lower) | (df[col] > upper)]\n",
    "        outlier_count = outliers.shape[0]\n",
    "        outlier_pct = (outlier_count / df.shape[0]) * 100\n",
    "        outlier_summary[col] = {\n",
    "            \"Lower Bound\": lower,\n",
    "            \"Upper Bound\": upper,\n",
    "            \"Outlier Count\": outlier_count,\n",
    "            \"Outlier %\": round(outlier_pct, 2)\n",
    "        }\n",
    "\n",
    "        plt.figure(figsize=(10, 1.5))\n",
    "        sns.boxplot(x=df[col])\n",
    "        plt.title(f'Boxplot of {col}')\n",
    "        plt.show()\n",
    "\n",
    "    return pd.DataFrame(outlier_summary).T\n",
    "\n",
    "outlier_report = detect_outliers(df, important_numerical_cols)\n",
    "\n",
    "outlier_report.sort_values(\"Outlier %\", ascending=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d73b5a25-e39b-4d37-854a-577ebf5736af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define threshold for top 1% of Incurred\n",
    "threshold_99 = df['Incurred'].quantile(0.99)\n",
    "\n",
    "# Filter top 1% claims\n",
    "outliers = df[df['Incurred'] >= threshold_99]\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(outliers['Incurred'], bins=30, color='crimson', kde=True)\n",
    "plt.title(\"Top 1% Most Expensive Claims (Incurred ≥ {:.2f})\".format(threshold_99))\n",
    "plt.xlabel(\"Incurred Amount\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.show()\n",
    "\n",
    "print(f\"Top 1% threshold: {threshold_99:.2f}\")\n",
    "print(f\"Number of outliers: {len(outliers)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e59df516-fe25-41a8-99df-c9cc34156464",
   "metadata": {},
   "source": [
    "#### Some explanation of the outlier results:\n",
    "\n",
    "* TP_injury_whiplash, 1438 claims (~18.7%) have this injury.\n",
    "* Outlier counts in the Incurred column (979 claims or 12.73%) indicate a significant number of claims lie outside expected normal ranges — these are candidates for further investigation or treatment in modeling (e.g., capping, transforming, or excluding). The negative values, despite showing financial gain for the insurance company, they could be even fraudulent transactions which need to be identified further.\n",
    "* Variables with very low outlier thershold: These are categorical or numeric variables with mostly expected values, and only a few unusual ones that could be data entry errors or very rare claim types.\n",
    "* Inception to loss has zero outliers: This means all values fall within the expected bounds. No outliers detected here.\n",
    "* **Note:** Outliers are important indicators which cannot be excluded from the dataframe, else we will miss important information. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41c5faa7-9f2d-44b2-bf13-e3179e8146bf",
   "metadata": {},
   "source": [
    "## EDA to better understand the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c03c492c-f819-4b5a-a196-06fc65e30c7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "sns.histplot(df['Notification_period'], bins=30, kde=True)\n",
    "plt.title(\"Distribution of Notification Period\")\n",
    "plt.xlabel(\"Notification Period (days)\")\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "sns.histplot(df['Incurred'], bins=50, color='blue', label='Incurred', alpha=0.6)\n",
    "sns.histplot(df['Capped Incurred'], bins=50, color='orange', label='Capped Incurred', alpha=0.6)\n",
    "plt.legend()\n",
    "plt.title(\"Distribution of Incurred vs Capped Incurred\")\n",
    "plt.xlabel(\"Claim Amount\")\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18576958-bedc-4415-a399-30d8198f4286",
   "metadata": {},
   "source": [
    "## Statistical Comparison between capped incured and whiplash injuries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "708c959c-b234-4ef2-a4f1-686d18b9eb88",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import ttest_ind, mannwhitneyu, kruskal\n",
    "\n",
    "whiplash_yes = df[df['TP_injury_whiplash'] == 1]['Incurred']\n",
    "whiplash_no = df[df['TP_injury_whiplash'] == 0]['Incurred']\n",
    "\n",
    "print(\"Mean Incurred Amount (Whiplash):\", whiplash_yes.mean())\n",
    "print(\"Mean Incurred Amount (No Whiplash):\", whiplash_no.mean())\n",
    "print(\"Median Incurred Amount (Whiplash):\", whiplash_yes.median())\n",
    "print(\"Median Incurred Amount (No Whiplash):\", whiplash_no.median())\n",
    "print(\"Sample Sizes:\", len(whiplash_yes), \"vs\", len(whiplash_no))\n",
    "\n",
    "grouped = [df[df['TP_injury_whiplash'] == cat]['Incurred'] for cat in sorted(df['TP_injury_whiplash'].unique())]\n",
    "\n",
    "stat, p = kruskal(*grouped)\n",
    "print(f\"Kruskal–Wallis H-statistic = {stat:.2f}, p-value = {p:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7672150-3984-4112-8ee1-32b83ba13fac",
   "metadata": {},
   "source": [
    "**Explanation:** We statistically compared claims with and without whiplash injuries using Kruskal statistical tests (comparing continuous variable with multi-class variable). The test confirmed that claims involving whiplash have significantly higher incurred costs than those without."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be2f28f7-f2e9-4fc5-9c54-af70931b01e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 6))\n",
    "sns.boxplot(x='TP_injury_whiplash', y='Incurred', data=df)\n",
    "plt.title(\"Incurred Amount by Whiplash Injury Class\")\n",
    "plt.xlabel(\"Whiplash Injury Code\")\n",
    "plt.ylabel(\"Incurred Amount\")\n",
    "plt.ylim(0, df['Incurred'].quantile(0.95))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b95bef7-40cc-40d3-bcd2-64615bb86e5f",
   "metadata": {},
   "source": [
    "## Claim frequency by hour of day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d305baf-6eac-4deb-b7eb-c0b70c17e737",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 5))\n",
    "sns.countplot(x='Time_hour', data=df, palette='viridis')\n",
    "plt.title(\"Number of Claims by Hour of Day\")\n",
    "plt.xlabel(\"Hour (24-hour format)\")\n",
    "plt.ylabel(\"Number of Claims\")\n",
    "plt.xticks(range(0, 24))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e1f24d4-270b-4880-89d4-b588dbc0643f",
   "metadata": {},
   "source": [
    "## Claims per region"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "378cacc2-2fc5-43c3-b601-be4c1f20602b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tp_region_cols = [col for col in df.columns if col.startswith('TP_region_')]\n",
    "region_counts = df[tp_region_cols].sum().sort_values(ascending=False)\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(x=region_counts.index, y=region_counts.values, palette=\"coolwarm\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.title(\"Claims Count by Region\")\n",
    "plt.ylabel(\"Number of Claims\")\n",
    "plt.xlabel(\"Region\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eefa9463-ada1-4739-8be6-14ae44099f5f",
   "metadata": {},
   "source": [
    "## Distribution of Incurred cost (Log scale)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e6fe1b0-a841-444c-be17-2d047d136b43",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 5))\n",
    "sns.histplot(np.log1p(df['Incurred']), bins=50, kde=True, color='green')\n",
    "plt.title(\"Log-Scaled Distribution of Incurred Claim Amount\")\n",
    "plt.xlabel(\"log(Incurred + 1)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17aa333d-4187-436f-b6a1-a15959d9a92b",
   "metadata": {},
   "source": [
    "**Insight:** Most claims are low-to-moderate cost; few are very large (heavy tail)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fd2e5f5-88ce-4da3-9e6e-4adc92e4fb72",
   "metadata": {},
   "source": [
    "## Incurred by Third party type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9941b786-b01a-4c3f-b736-fde937e2542c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tp_types = ['TP_type_driver', 'TP_type_pedestrian', 'TP_type_cyclist', 'TP_type_bike']\n",
    "melted = df.melt(id_vars='Incurred', value_vars=tp_types, var_name='TP_Type', value_name='Present')\n",
    "melted = melted[melted['Present'] == 1]\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.boxplot(x='TP_Type', y='Incurred', data=melted)\n",
    "plt.ylim(0, df['Incurred'].quantile(0.95))\n",
    "plt.title(\"Claim Cost by Third Party Type\")\n",
    "plt.xticks(rotation=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b18b850f-7fef-44c9-a463-68dedff20c72",
   "metadata": {},
   "source": [
    "## Notification delay vs cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a01e7b8-5270-4633-96bf-50f311dcdf45",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "sns.scatterplot(x='Notification_period', y='Incurred', data=df, alpha=0.3)\n",
    "plt.ylim(0, df['Incurred'].quantile(0.95))\n",
    "plt.title(\"Notification Delay vs. Incurred Claim Amount\")\n",
    "plt.xlabel(\"Days Between Incident and Notification\")\n",
    "plt.ylabel(\"Incurred\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef53e5b7-070b-4d76-9496-fb80b2075161",
   "metadata": {},
   "source": [
    "**Note:** Majority of incidents have been incurred within the first 200 days from the notification and incident"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5792ac70-610b-4e8a-9674-dded437a53c1",
   "metadata": {},
   "source": [
    "## Proportion of Injuries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32a65775-e524-4151-a2bf-0049821a43e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "injury_cols = [col for col in df.columns if col.startswith(\"TP_injury_\")]\n",
    "injury_counts = df[injury_cols].sum().sort_values(ascending=False)\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "sns.barplot(x=injury_counts.index, y=injury_counts.values, palette='rocket')\n",
    "plt.title(\"Frequency of Injury Types\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.ylabel(\"Number of Claims\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0dd95e6-ef4d-4062-b22e-179af210cdd9",
   "metadata": {},
   "source": [
    "## Notification Delay vs Incurred by Fault"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54384471-a015-4fdb-bf83-1b7ff9c70863",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "sns.scatterplot(data=df, x='Notification_period', y='Incurred',\n",
    "                hue='PH_considered_TP_at_fault', alpha=0.5, palette='coolwarm')\n",
    "plt.ylim(0, df['Incurred'].quantile(0.95))\n",
    "plt.title(\"Incurred vs. Notification Delay (Colored by At Fault)\")\n",
    "plt.xlabel(\"Notification Period (days)\")\n",
    "plt.ylabel(\"Incurred\")\n",
    "plt.legend(title=\"TP At Fault (1=True)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdfe6bd5-1c4b-4685-b03f-47bb7be7e8ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "df['year'] = df['date_of_loss'].dt.year\n",
    "df['month'] = df['date_of_loss'].dt.month\n",
    "df['day'] = df['date_of_loss'].dt.day\n",
    "df['weekday'] = df['date_of_loss'].dt.day_name()\n",
    "\n",
    "monthly_counts = df.groupby(pd.Grouper(key='date_of_loss', freq='M')).size()\n",
    "\n",
    "monthly_counts.plot(figsize=(12,6), title='Number of Claims per Month')\n",
    "plt.xlabel('Month')\n",
    "plt.ylabel('Number of Claims')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d2b65ba-050e-4e8a-bf24-14ae19ddc83e",
   "metadata": {},
   "outputs": [],
   "source": [
    "yearly_counts = df.groupby(pd.Grouper(key='date_of_loss', freq='Y')).size()\n",
    "\n",
    "yearly_counts.plot(figsize=(12,6), title='Number of Claims per Year')\n",
    "plt.xlabel('Month')\n",
    "plt.ylabel('Number of Claims')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5898ad01-42ba-4d93-aeb6-f52cdbf13888",
   "metadata": {},
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfa1cc3b-5ea0-41a0-85e4-03dd404d97a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Late_Report'] = df['Notification_period'] > 7\n",
    "\n",
    "urban_regions = ['TP_region_london', 'TP_region_outerldn', 'TP_region_westmid']\n",
    "df['Urban_Region'] = df[urban_regions].sum(axis=1) > 0\n",
    "\n",
    "threshold_95 = df['Incurred'].quantile(0.95)\n",
    "df['High_Cost_Claim'] = df['Incurred'] >= threshold_95\n",
    "\n",
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c74a8c2e-d197-47df-8d83-0e37e355b797",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(x='Late_Report', hue='High_Cost_Claim', data=df)\n",
    "plt.title(\"High Cost Claims by Late Reporting Status\")\n",
    "plt.xlabel(\"Late Report\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.show()\n",
    "\n",
    "sns.boxplot(x='Urban_Region', y='Incurred', data=df)\n",
    "plt.title(\"Incurred Amounts in Urban vs Non-Urban Regions\")\n",
    "plt.ylim(0, df['Incurred'].quantile(0.95))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76210c32-715c-4dc7-b5df-fb07abf8c978",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Time_Bucket'] = pd.cut(df['Time_hour'],\n",
    "                           bins=[-1, 6, 12, 17, 21, 24],\n",
    "                           labels=['Night', 'Morning', 'Afternoon', 'Evening', 'Late night'])\n",
    "\n",
    "df['Notification_Delay_Bucket'] = pd.cut(df['Notification_period'],\n",
    "                                         bins=[-10, 0, 3, 7, 100],\n",
    "                                         labels=['Early', 'Fast', 'Late', 'Very Late'])\n",
    "df['Multiple_Injuries'] = df[[col for col in df.columns if col.startswith(\"TP_injury_\")]].sum(axis=1) > 1\n",
    "df['TP_Type_Count'] = df[[col for col in df.columns if col.startswith(\"TP_type_\")]].sum(axis=1)\n",
    "df['Log_Incurred'] = np.log1p(df['Incurred'])\n",
    "df['Weekend_Incident'] = pd.to_datetime(df['date_of_loss']).dt.dayofweek >= 5\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75914a98-5fda-48c2-8594-b513cb135f3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_cols = df.select_dtypes(include=['number']).columns.tolist()\n",
    "numerical_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7e462ff-a2bd-41ac-afc0-b82afafb96ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Claim Seasonality Heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb70c80b-9d7f-4fad-8b2a-ca361c79338d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['month'] = df['date_of_loss'].dt.month\n",
    "df['weekday'] = df['date_of_loss'].dt.day_name()\n",
    "\n",
    "pivot = df.pivot_table(index='weekday', columns='month', values='Incurred', aggfunc='count')\n",
    "sns.heatmap(pivot, cmap='YlGnBu')\n",
    "\n",
    "monthly_stats = df.groupby('month').agg({\n",
    "    'Incurred': ['sum', 'mean', 'count']\n",
    "}).reset_index()\n",
    "\n",
    "monthly_stats.columns = ['Month', 'Total_Cost', 'Average_Cost', 'Claim_Count']\n",
    "\n",
    "plt.figure(figsize=(12,6))\n",
    "plt.plot(monthly_stats['Month'], monthly_stats['Total_Cost'], marker='o', label='Total Incurred')\n",
    "plt.title('Total Claim Cost Over Time')\n",
    "plt.xlabel('Month')\n",
    "plt.ylabel('Total Incurred')\n",
    "plt.xticks(rotation=45)\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(12,6))\n",
    "plt.plot(monthly_stats['Month'], monthly_stats['Average_Cost'], marker='o', color='orange', label='Average Incurred')\n",
    "plt.title('Average Claim Cost Over Time')\n",
    "plt.xlabel('Month')\n",
    "plt.ylabel('Average Incurred')\n",
    "plt.xticks(rotation=45)\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(12,6))\n",
    "plt.bar(monthly_stats['Month'], monthly_stats['Claim_Count'], color='gray')\n",
    "plt.title('Number of Claims per Month')\n",
    "plt.xlabel('Month')\n",
    "plt.ylabel('Claim Count')\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5652a9d1-c892-4494-8fa0-c87f4dee42af",
   "metadata": {},
   "source": [
    "## Label Encoding with one hot encoding\n",
    "\n",
    "**One-Hot Encoding treats categories as independent and unordered, preventing the model from assuming any hierarchy.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fd4cc4b-9661-49f4-b8bc-aea76f68ffb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_cols = df.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "\n",
    "df = pd.get_dummies(df, columns=categorical_cols, drop_first=False)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cfbbb1d-5cc3-4fe8-b7ee-c39bdd5f534f",
   "metadata": {},
   "outputs": [],
   "source": [
    "bool_cols = df.select_dtypes(include='bool').columns\n",
    "df[bool_cols] = df[bool_cols].astype(int)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f2c86e9-a728-42c7-beef-a45b35776b4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9e5d2b3-4763-4cad-abee-53eb1a9d0c9d",
   "metadata": {},
   "source": [
    "## Correlation Heatmap (helps in removing multicolinearity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7003717c-89df-4f0a-8c6a-98af63b4ad85",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_size = 10\n",
    "\n",
    "cols = df.columns\n",
    "\n",
    "num_chunks = 5\n",
    "\n",
    "for i in range(num_chunks):\n",
    "    start_idx = i * chunk_size\n",
    "    end_idx = start_idx + chunk_size\n",
    "    cols_subset = cols[start_idx:end_idx]\n",
    "    \n",
    "    corr = df[cols_subset].corr()\n",
    "    \n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(corr, annot=True, fmt=\".2f\", cmap='coolwarm', square=True, cbar_kws={\"shrink\": .8})\n",
    "    plt.title(f'Correlation Heatmap: Columns {start_idx} to {end_idx-1}')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aa84112-1a89-4d6e-9dcb-6b28f39845f4",
   "metadata": {},
   "source": [
    "## Outlier Handling with Robust Scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02c1dad7-c556-44ae-86b7-59d462887e31",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import RobustScaler\n",
    "\n",
    "scaler = RobustScaler()\n",
    "\n",
    "num_cols = ['Notification_period',\n",
    "             'Inception_to_loss',\n",
    "             'Time_hour',\n",
    "             'Vechile_registration_present',\n",
    "             'Incident_details_present',\n",
    "             'Injury_details_present',\n",
    "             'TP_type_insd_pass_back',\n",
    "             'TP_type_insd_pass_front',\n",
    "             'TP_type_driver',\n",
    "             'TP_type_pass_back',\n",
    "             'TP_type_pass_front',\n",
    "             'TP_type_bike',\n",
    "             'TP_type_cyclist',\n",
    "             'TP_type_pass_multi',\n",
    "             'TP_type_pedestrian',\n",
    "             'TP_type_other',\n",
    "             'TP_type_nk',\n",
    "             'TP_injury_whiplash',\n",
    "             'TP_injury_traumatic',\n",
    "             'TP_injury_fatality',\n",
    "             'TP_injury_unclear',\n",
    "             'TP_injury_nk',\n",
    "             'TP_region_eastang',\n",
    "             'TP_region_eastmid',\n",
    "             'TP_region_london',\n",
    "             'TP_region_north',\n",
    "             'TP_region_northw',\n",
    "             'TP_region_outerldn',\n",
    "             'TP_region_scotland',\n",
    "             'TP_region_southe',\n",
    "             'TP_region_southw',\n",
    "             'TP_region_wales',\n",
    "             'TP_region_westmid',\n",
    "             'TP_region_yorkshire',\n",
    "             'TP_Type_Count']\n",
    "\n",
    "df[num_cols] = scaler.fit_transform(df[num_cols])\n",
    "\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df9117e2-250e-4219-8d0d-d0f77e6d379c",
   "metadata": {},
   "source": [
    "## Drop multi-collinear features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98b9eb4d-0a7c-4184-b631-78d3c24e463c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_multicollinear_features(df, threshold=0.6):\n",
    "    \n",
    "    corr_matrix = df.corr().abs()\n",
    "    upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n",
    "    to_drop = [col for col in upper.columns if any(upper[col] > threshold)]\n",
    "    \n",
    "    print(f\"Columns to drop due to multicollinearity (corr > {threshold}): {to_drop}\")\n",
    "    \n",
    "    return df.drop(columns=to_drop)\n",
    "\n",
    "df_clean = remove_multicollinear_features(df[num_cols], threshold=0.6)\n",
    "df_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "089c8489-0662-457e-b3e9-ff26cae6f22b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8c17f0a-e1e0-49eb-a425-8635daaec84b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f04ca1a-a35f-4620-873d-53b0f5ec087d",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_add = [col for col in df.columns if col not in df_clean.columns]\n",
    "\n",
    "df_extra = df[cols_to_add]\n",
    "\n",
    "df_combined = pd.concat([df_clean.reset_index(drop=True), df_extra.reset_index(drop=True)], axis=1)\n",
    "df_combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "def963d6-a40c-4b10-a0d9-d23d2a0d9cdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0792b23-a6ad-4cf2-9202-252a00ff7548",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Incurred'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f49da40a-5319-4543-bec1-0892d02533cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby(['Weather_conditions_NORMAL', 'Urban_Region'])['Incurred'].mean().unstack().plot(kind='bar')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f0c00c1-8adb-479a-9459-eeb895def3ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby(['Weather_conditions_SNOW,ICE,FOG', 'Urban_Region'])['Incurred'].mean().unstack().plot(kind='bar')\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79435cfd-00e5-48c0-8a2c-ede8aa4d0882",
   "metadata": {},
   "source": [
    "# Target Variable Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1f79e12-a401-405f-b42d-12c2f1ca37f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_zeros = (df['Incurred'] == 0).sum()\n",
    "percent_zeros = 100 * num_zeros / len(df)\n",
    "\n",
    "print(f\"Number of zero Incurred values: {num_zeros}\")\n",
    "print(f\"Percentage of zeros: {percent_zeros:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4f36989-5271-4b5f-87ab-3d8333346477",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "545d2af3-80e1-47e0-9b92-62d049bbeabb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Incurred'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dc3f638-09a6-4b4d-9263-7659ffb69492",
   "metadata": {},
   "source": [
    "# 1. Regression Analysis (Machine Learning and best model selection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eeafe26-b5e0-4b79-9cb1-698ee9998799",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import GradientBoostingRegressor, RandomForestRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "X = df.drop(columns=['Incurred', 'Capped Incurred', 'Log_Incurred', 'date_of_loss'])\n",
    "y = np.log1p(df['Incurred'])\n",
    "y = y.fillna(0)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "models = {\n",
    "    \"Gradient Boosting\": GradientBoostingRegressor(random_state=42),\n",
    "    \"Random Forest\": RandomForestRegressor(random_state=42),\n",
    "    \"XGBoost\": XGBRegressor(random_state=42)\n",
    "}\n",
    "\n",
    "results = {}\n",
    "\n",
    "for name, model in models.items():\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    results[name] = {\n",
    "        'MSE': mean_squared_error(y_test, y_pred),\n",
    "        'MAE': mean_absolute_error(y_test, y_pred),\n",
    "        'RMSE': np.sqrt(mean_squared_error(y_test, y_pred)),\n",
    "        'R2': r2_score(y_test, y_pred)\n",
    "    }\n",
    "\n",
    "results_df = pd.DataFrame(results).T.sort_values(by='RMSE')\n",
    "print(results_df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ddbf77d-f40f-4f3e-8910-9d5346324ec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GradientBoostingRegressor().fit(X_train, y_train)\n",
    "importances = model.feature_importances_\n",
    "feat_importance = pd.Series(importances, index=X.columns).sort_values(ascending=False)\n",
    "\n",
    "feat_importance.head(20).plot(kind='barh', figsize=(10, 8), title=\"Top Feature Importances\")\n",
    "plt.gca().invert_yaxis()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f80f26ff-ceac-4016-8b46-4984e7acacf0",
   "metadata": {},
   "source": [
    "## Retrain model Gradient Boosting with top 10, 20, 25, 30 features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9654028e-bec0-47f9-aa00-550accfe57b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_features = feat_importance.head(10).index\n",
    "\n",
    "X_train_top = X_train[top_features]\n",
    "X_test_top = X_test[top_features]\n",
    "\n",
    "model_top = GradientBoostingRegressor(random_state=42)\n",
    "model_top.fit(X_train_top, y_train)\n",
    "\n",
    "y_pred_top = model_top.predict(X_test_top)\n",
    "\n",
    "mae = mean_absolute_error(y_test, y_pred_top)\n",
    "mse = mean_squared_error(y_test, y_pred_top)\n",
    "r2 = r2_score(y_test, y_pred_top)\n",
    "\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "\n",
    "print(f\"Top-10 Features Model Performance:\")\n",
    "print(f\"MAE:  {mae:.4f}\")\n",
    "print(f\"MSE:  {mse:.4f}\")\n",
    "print(f\"R2:   {r2:.4f}\")\n",
    "print(f\"RMSE: {rmse: .4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69ef83c6-65d0-4bf8-bbd7-d4a7d44b450f",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_features = feat_importance.head(20).index\n",
    "\n",
    "X_train_top = X_train[top_features]\n",
    "X_test_top = X_test[top_features]\n",
    "\n",
    "model_top = GradientBoostingRegressor(random_state=42)\n",
    "model_top.fit(X_train_top, y_train)\n",
    "\n",
    "y_pred_top = model_top.predict(X_test_top)\n",
    "\n",
    "mae = mean_absolute_error(y_test, y_pred_top)\n",
    "mse = mean_squared_error(y_test, y_pred_top)\n",
    "r2 = r2_score(y_test, y_pred_top)\n",
    "\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "\n",
    "print(f\"Top-20 Features Model Performance:\")\n",
    "print(f\"MAE:  {mae:.4f}\")\n",
    "print(f\"MSE:  {mse:.4f}\")\n",
    "print(f\"R2:   {r2:.4f}\")\n",
    "print(f\"RMSE: {rmse: .4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a462f756-a779-429d-a5be-03e33961ec36",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_features = feat_importance.head(25).index\n",
    "\n",
    "X_train_top = X_train[top_features]\n",
    "X_test_top = X_test[top_features]\n",
    "\n",
    "model_top = GradientBoostingRegressor(random_state=42)\n",
    "model_top.fit(X_train_top, y_train)\n",
    "\n",
    "y_pred_top = model_top.predict(X_test_top)\n",
    "\n",
    "mae = mean_absolute_error(y_test, y_pred_top)\n",
    "mse = mean_squared_error(y_test, y_pred_top)\n",
    "r2 = r2_score(y_test, y_pred_top)\n",
    "\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "\n",
    "print(f\"Top-25 Features Model Performance:\")\n",
    "print(f\"MAE:  {mae:.4f}\")\n",
    "print(f\"MSE:  {mse:.4f}\")\n",
    "print(f\"R2:   {r2:.4f}\")\n",
    "print(f\"RMSE: {rmse: .4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddd87297-bc14-46b0-a7b9-399905ff4847",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_features = feat_importance.head(30).index\n",
    "\n",
    "X_train_top = X_train[top_features]\n",
    "X_test_top = X_test[top_features]\n",
    "\n",
    "model_top = GradientBoostingRegressor(random_state=42)\n",
    "model_top.fit(X_train_top, y_train)\n",
    "\n",
    "y_pred_top = model_top.predict(X_test_top)\n",
    "\n",
    "mae = mean_absolute_error(y_test, y_pred_top)\n",
    "mse = mean_squared_error(y_test, y_pred_top)\n",
    "r2 = r2_score(y_test, y_pred_top)\n",
    "\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "\n",
    "print(f\"Top-30 Features Model Performance:\")\n",
    "print(f\"MAE:  {mae:.4f}\")\n",
    "print(f\"MSE:  {mse:.4f}\")\n",
    "print(f\"R2:   {r2:.4f}\")\n",
    "print(f\"RMSE: {rmse: .4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9caca6eb-7f94-4904-9fc1-e1fc7846736d",
   "metadata": {},
   "source": [
    "# Trying Grid Search for model improvement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78431275-00c6-426b-ad58-a4c5634b6ad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "params = {\n",
    "    'n_estimators': [100, 200],\n",
    "    'max_depth': [3, 5],\n",
    "    'learning_rate': [0.05, 0.1],\n",
    "}\n",
    "\n",
    "grid = GridSearchCV(GradientBoostingRegressor(random_state=42), params, cv=3, scoring='neg_root_mean_squared_error')\n",
    "grid.fit(X_train, y_train)\n",
    "print(\"Best RMSE:\", -grid.best_score_)\n",
    "print(\"Best Params:\", grid.best_params_)\n",
    "\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "mape = np.mean(np.abs((y_test - y_pred) / y_test)) * 100\n",
    "\n",
    "print(\"Best Parameters:\", grid.best_params_)\n",
    "print(f\"MAE:  {mae:.4f}\")\n",
    "print(f\"MSE:  {mse:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f5be2e0-82e3-46ba-a449-77a7942d7d5c",
   "metadata": {},
   "source": [
    "# Executive Summary: Predicting Incurred Claim Amounts Using Machine Learning\n",
    "\n",
    "## Objective\n",
    "\n",
    "The goal of this project was to build a regression model to accurately predict the **incurred claim amount** for insurance cases using structured claim-level features. Accurately forecasting incurred amounts is critical for:\n",
    "\n",
    "- **Reserving accuracy**\n",
    "- **Fraud detection**\n",
    "- **Resource allocation**\n",
    "- **Premium pricing**\n",
    "\n",
    "---\n",
    "\n",
    "## Modeling Approach\n",
    "\n",
    "We tested multiple algorithms and configurations:\n",
    "- Models: **Gradient Boosting**, **Random Forest**, and **XGBoost**\n",
    "- Feature selection: Top 10, 20, 25, and 30 features based on importance\n",
    "- Hyperparameter tuning using **GridSearchCV**\n",
    "- Evaluation metrics:\n",
    "  - **MAE** (Mean Absolute Error)\n",
    "  - **MSE** (Mean Squared Error)\n",
    "  - **RMSE** (Root Mean Squared Error)\n",
    "  - **R²** (Coefficient of Determination)\n",
    "\n",
    "---\n",
    "\n",
    "## Key Results\n",
    "\n",
    "### Model Performance\n",
    "\n",
    "| Model               | MAE    | MSE     | RMSE   | R²      |\n",
    "|---------------------|--------|---------|--------|---------|\n",
    "| Gradient Boosting   | 2.6037 | 9.7264  | 3.1187 | 0.2998  |\n",
    "| Random Forest       | 2.5873 | 9.9813  | 3.1593 | 0.2814  |\n",
    "| XGBoost (default)   | 2.6351 | 10.4644 | 3.2349 | 0.2467  |\n",
    "| XGBoost (GridSearch)| 2.6351 | 10.4644 | **3.0701** | ~     |\n",
    "\n",
    "### Feature Selection Performance\n",
    "\n",
    "| Feature Selection   | MAE    | MSE     | RMSE   | R²      |\n",
    "|---------------------|--------|---------|--------|---------|\n",
    "| Top-10 Features     | 2.6448 | 9.9464  | 3.2349 | 0.2839  |\n",
    "| Top-20 Features     | 2.6087 | 9.7660  | 3.2349 | 0.2969  |\n",
    "| Top-25 Features     | 2.5989 | 9.6996  | 3.2349 | **0.3017**  |\n",
    "| Top-30 Features     | 2.6047 | 9.7453  | 3.2349 | 0.2984  |\n",
    "\n",
    "- **Best overall performance**: XGBoost (GridSearchCV) with RMSE = **3.0701**\n",
    "\n",
    "---\n",
    "\n",
    "## Business Impact\n",
    "\n",
    "Accurately predicting incurred amounts provides several key benefits:\n",
    "\n",
    "1. **Improved Claim Reserving**  \n",
    "   Better estimation of future liabilities and financial provisioning.\n",
    "\n",
    "2. **Fraud Detection Support**  \n",
    "   Discrepancies between predicted and actual incurred values flag potentially suspicious claims.\n",
    "\n",
    "3. **Underwriting and Pricing Accuracy**  \n",
    "   Enhances actuarial models and supports risk-based pricing strategies.\n",
    "\n",
    "4. **Operational Efficiency**  \n",
    "   Helps prioritize complex or high-value claims automatically.\n",
    "\n",
    "5. **Scenario Modeling**  \n",
    "   Allows simulation of financial exposure under different claim distributions or policy changes.\n",
    "\n",
    "---\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "The optimized XGBoost model with tuned hyperparameters yielded the best performance with an RMSE of **3.0701** and R² around **0.30**. This suggests that ~30% of the variance in incurred claim amounts is explained by the features available.\n",
    "\n",
    "Future improvements can be made by incorporating additional data sources such as:\n",
    "- NLP-derived insights from claim narratives\n",
    "- Geographic and behavioral risk factors\n",
    "- Policyholder history or segmentation\n",
    "\n",
    "This project lays a strong foundation for **data-driven, scalable, and automated claims management**, contributing to reduced loss ratios, better reserve management, and improved strategic forecasting.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13f93816-72a3-41ee-a07a-906a912ceec0",
   "metadata": {},
   "source": [
    "# 2. Binary Classification Prediction on High cost Claims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0840f4f7-5f29-4d0b-9f4f-6d39b4ff3c43",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['High_Cost_Claim'].unique()\n",
    "print(df['High_Cost_Claim'].value_counts())\n",
    "print(df['High_Cost_Claim'].value_counts(normalize=True))\n",
    "print(\"Unique classes:\", df['High_Cost_Claim'].unique())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77f20265-b10b-4fce-b6f5-cde931c514b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score, precision_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "X = df.drop(columns=['High_Cost_Claim', 'Incurred', 'Capped Incurred', 'Log_Incurred', 'date_of_loss'])\n",
    "y = df['High_Cost_Claim']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb0ceb0b-4553-454e-8a31-af39650d0a2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72487e53-b99f-4861-bfc5-c9f47c667c36",
   "metadata": {},
   "outputs": [],
   "source": [
    "smote = SMOTE(random_state=42)\n",
    "X_train, y_train = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "print(\"After SMOTE:\")\n",
    "print(y_train.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04d59530-43f0-4d50-b6bf-a6b9ec19ded6",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {\n",
    "    \"Logistic Regression\": LogisticRegression(max_iter=5000),\n",
    "    \"Gradient Boosting\": GradientBoostingClassifier(),\n",
    "    \"XGBoost\": XGBClassifier(use_label_encoder=False, eval_metric='logloss'),\n",
    "    \"SVC\": SVC(probability=True),\n",
    "    \"Random Forest\": RandomForestClassifier(random_state=42)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e08e9f3d-4c18-453f-a0b9-00d00ec14d23",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "\n",
    "for name, model in models.items():\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_proba = model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    auc = roc_auc_score(y_test, y_proba)\n",
    "\n",
    "    results.append({\n",
    "        'Model': name,\n",
    "        'Accuracy': acc,\n",
    "        'Precision': precision,\n",
    "        'F1 Score': f1,\n",
    "        'AUC': auc\n",
    "    })\n",
    "results_df = pd.DataFrame(results).sort_values(by='AUC', ascending=False)\n",
    "results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "609fc32a-a67b-4cbe-bbd7-4d0a58112771",
   "metadata": {},
   "source": [
    "# Hyperparameter tuning of our best model Gradient Boosting for improved predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fd58b25-5c6f-43f7-955c-9282b3d689b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid_gb = {\n",
    "    \"n_estimators\": [100, 200, 300],\n",
    "    \"learning_rate\": [0.01, 0.05, 0.1],\n",
    "    \"max_depth\": [3, 5, 7],\n",
    "    \"subsample\": [0.8, 1.0]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fb9e491-62d0-4dc0-b608-b995f423c5d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "gb_model = GradientBoostingClassifier(random_state=42)\n",
    "\n",
    "grid_search_gb = GridSearchCV(\n",
    "    estimator=gb_model,\n",
    "    param_grid=param_grid_gb,\n",
    "    scoring='f1',\n",
    "    cv=3,\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "grid_search_gb.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b460e46b-c059-4d82-a741-2b8aa7800179",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_gb = grid_search_gb.best_estimator_\n",
    "y_pred = best_gb.predict(X_test)\n",
    "y_proba = best_gb.predict_proba(X_test)[:, 1]\n",
    "\n",
    "print(grid_search_gb.best_params_)\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"Precision:\", precision_score(y_test, y_pred))\n",
    "print(\"F1 Score:\", f1_score(y_test, y_pred))\n",
    "print(\"AUC:\", roc_auc_score(y_test, y_proba))\n",
    "\n",
    "results_df.loc[results_df['Model'] == 'Gradient Boosting', ['Accuracy', 'Precision', 'F1 Score', 'AUC']] = [\n",
    "    accuracy_score(y_test, y_pred),\n",
    "    precision_score(y_test, y_pred),\n",
    "    f1_score(y_test, y_pred),\n",
    "    roc_auc_score(y_test, y_proba)\n",
    "]\n",
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69731484-a2f1-4320-95c1-2a76c15a452e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import RFE\n",
    "\n",
    "model = GradientBoostingClassifier(random_state=42)\n",
    "metrics = []\n",
    "\n",
    "for k in range(5, X_train.shape[1], 5):\n",
    "    rfe = RFE(estimator=model, n_features_to_select=k)\n",
    "    rfe.fit(X_train, y_train)\n",
    "    \n",
    "    X_train_rfe = X_train.loc[:, rfe.support_]\n",
    "    X_test_rfe = X_test.loc[:, rfe.support_]\n",
    "    \n",
    "    model.fit(X_train_rfe, y_train)\n",
    "    y_pred = model.predict(X_test_rfe)\n",
    "    y_proba = model.predict_proba(X_test_rfe)[:, 1]\n",
    "\n",
    "    metrics.append({\n",
    "        \"Num Features\": k,\n",
    "        \"Accuracy\": accuracy_score(y_test, y_pred),\n",
    "        \"Precision\": precision_score(y_test, y_pred),\n",
    "        \"F1 Score\": f1_score(y_test, y_pred),\n",
    "        \"AUC\": roc_auc_score(y_test, y_proba)\n",
    "    })\n",
    "\n",
    "results_df = pd.DataFrame(metrics)\n",
    "print(results_df.sort_values(by='F1 Score', ascending=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1003ec6-068f-4524-9ef1-33c4e3513cb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_25_features = X_train.columns[rfe.support_][:25]\n",
    "best_25_features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11225509-5877-43ac-87e2-0b148071ea40",
   "metadata": {},
   "source": [
    "# Retrain on the top 25 features (final model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b4e05cd-1b73-4486-a0b9-d5cca39371f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_top = X_train[best_25_features]\n",
    "X_test_top = X_test[best_25_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55b0a39d-5e83-426d-b226-3810ff90af72",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "final_model = GradientBoostingClassifier(random_state=42)\n",
    "final_model.fit(X_train_top, y_train)\n",
    "\n",
    "y_pred = final_model.predict(X_test_top)\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "857b5acb-24a1-477d-9dff-c2574998af6d",
   "metadata": {},
   "source": [
    "## Feature interpretability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fe3fc2a-25a5-48c8-90fa-f42379b99064",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap\n",
    "\n",
    "explainer = shap.Explainer(final_model, X_train_top)\n",
    "shap_values = explainer(X_test_top)\n",
    "\n",
    "shap.summary_plot(shap_values, X_test_top, plot_type=\"bar\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6302c94-e290-4bf7-8b23-bafa3e86fb92",
   "metadata": {},
   "source": [
    "# Report of the Classification Problem"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98d86969-36a7-415a-acca-174dfcc93862",
   "metadata": {},
   "source": [
    "# Executive Summary: Predicting High-Cost Insurance Claims (Classification)\n",
    "\n",
    "## Objective\n",
    "\n",
    "The goal of this classification task was to identify **high-cost insurance claims** early in the claim lifecycle using machine learning. Flagging such claims is crucial to proactively manage risk, improve cost control, and optimize claims handling operations.\n",
    "\n",
    "---\n",
    "\n",
    "## Modeling Approach\n",
    "\n",
    "We approached the problem as a **binary classification**:\n",
    "- **Class 1**: High-cost claims (top 1% based on incurred amount)\n",
    "- **Class 0**: Normal claims\n",
    "\n",
    "### Key Steps:\n",
    "- Used models: **Gradient Boosting**, **XGBoost**, **Random Forest**, **Logistic Regression**, and **Support Vector Classifier (SVC)**\n",
    "- Conducted **GridSearchCV** hyperparameter tuning\n",
    "- Applied **Recursive Feature Elimination (RFE)** to optimize feature subset\n",
    "- Final model retrained with **Top 25 features** (best trade-off)\n",
    "\n",
    "---\n",
    "\n",
    "## Model Performance Summary\n",
    "\n",
    "### Best Performing Model: Gradient Boosting (Top 25 features)\n",
    "\n",
    "| Metric       | Value     |\n",
    "|--------------|-----------|\n",
    "| Accuracy     | **0.9480** |\n",
    "| Precision    | **0.5100** |\n",
    "| Recall       | **0.2900** |\n",
    "| F1 Score     | **0.3700** |\n",
    "| AUC-ROC      | **0.9054** |\n",
    "\n",
    "- **Hyperparameters**:  \n",
    "  `learning_rate = 0.1`, `max_depth = 3`, `n_estimators = 200`, `subsample = 0.8`\n",
    "\n",
    "### Performance Breakdown (Class 1: High-Cost Claims)\n",
    "\n",
    "| Metric    | Explanation |\n",
    "|-----------|-------------|\n",
    "| **Precision = 0.51** | Out of all predicted high-cost claims, 51% were correct. Acceptable false positive rate. |\n",
    "| **Recall = 0.29** | Out of actual high-cost claims, only 29% were detected. Needs improvement. |\n",
    "| **F1 Score = 0.37** | Reflects imbalance between precision and recall. |\n",
    "| **AUC = 0.9054** | High model separability overall, good signal in data. |\n",
    "\n",
    "---\n",
    "\n",
    "## Model Comparison Table (Updated)\n",
    "\n",
    "| Model               | Accuracy  | Precision | F1 Score | AUC     |\n",
    "|---------------------|-----------|-----------|----------|---------|\n",
    "| Logistic Regression | 0.9467    | 0.5294    | 0.3051   | **0.9111**  |\n",
    "| Gradient Boosting   | 0.9480    | 0.5526    | **0.3443** | 0.9110  |\n",
    "| Random Forest       | **0.9487**| **0.7273**| 0.1684   | 0.9021  |\n",
    "| XGBoost             | 0.9448    | 0.4815    | 0.2342   | 0.8691  |\n",
    "| SVC                 | 0.9454    | 0.0000    | 0.0000   | 0.5445  |\n",
    "\n",
    "> ✳️ *Gradient Boosting* offers the best **F1 Score** which means a better overall recall, while *Random Forest* achieves the highest **Precision** and **Accuracy**. *Logistic Regression* has the best **AUC**.\n",
    "\n",
    "---\n",
    "\n",
    "## 🏁 Feature Selection Performance (RFE Results, 10–35 Features)\n",
    "\n",
    "| # Features | Accuracy | Precision | F1 Score | AUC     |\n",
    "|------------|----------|-----------|----------|---------|\n",
    "| 10         | 0.9461   | 0.5094    | **0.3942** | **0.9198**  |\n",
    "| 15         | 0.9441   | 0.4783    | 0.3385   | 0.9170  |\n",
    "| 20         | 0.9454   | 0.5000    | 0.3636   | 0.9128  |\n",
    "| 25         | 0.9428   | 0.4615    | 0.3529   | 0.9055  |\n",
    "| 30         | 0.9402   | 0.4259    | 0.3333   | 0.9078  |\n",
    "| 35         | 0.9441   | 0.4821    | 0.3857   | 0.9088  |\n",
    "\n",
    "> *10 features* yielded the best **F1 Score** and **AUC**.  \n",
    "> *20-25 features* were selected as a balance between performance and interpretability.\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "## Business Impact\n",
    "\n",
    "Predicting high-cost claims enables:\n",
    "\n",
    "1. **Early Risk Detection (maybe Fraud!)**  \n",
    "   Flagging costly claims enables proactive case handling, investigations, or negotiation strategies.\n",
    "\n",
    "2. **Improved Reserves Planning**  \n",
    "   Enhances financial accuracy by estimating potential large payouts earlier.\n",
    "\n",
    "3. **Resource Prioritization**  \n",
    "   Allows fast-track assignment to senior claims handlers or fraud detection teams.\n",
    "\n",
    "4. **Operational Efficiency**  \n",
    "   Reduces time-to-resolution and administrative burden on predictable claims.\n",
    "\n",
    "5. **Strategic Underwriting**  \n",
    "   Helps inform risk selection and policy design based on learnings from frequent high-cost claim profiles.\n",
    "\n",
    "---\n",
    "\n",
    "## Areas for Improvement\n",
    "\n",
    "- **Low Recall (0.29)** suggests many high-cost claims are still being missed.\n",
    "- Future steps:\n",
    "  - **Expand dataset** beyond top 1% cutoff\n",
    "  - Include **external and unstructured data** (e.g., NLP from claim notes)\n",
    "  - Fine-tune **classification thresholds** to increase recall while managing precision\n",
    "\n",
    "---\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "The Gradient Boosting Classifier, with tuned parameters and top 20 features, offered the best overall performance. While precision is decent, recall is still low, meaning the model **cautiously predicts** high-cost claims. With further data and modeling iterations, this framework can be refined to **reduce financial exposure** and **improve claims workflow efficiency** across the organization.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9172afd1-26ca-45cd-a7a8-0ce9b19f1fbb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
