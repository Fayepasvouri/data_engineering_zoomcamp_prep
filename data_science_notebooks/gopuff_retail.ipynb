{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8f17118b",
   "metadata": {},
   "source": [
    "## Import Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "081eccb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "add108f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(file_path):\n",
    "    return pd.read_csv(file_path) \n",
    "\n",
    "impressions = load_data('impressions.csv')\n",
    "products = load_data('product_catalog.csv')\n",
    "transactions = load_data('transactions.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "638a8e0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "impressions.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fb730c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "products.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfce704a",
   "metadata": {},
   "outputs": [],
   "source": [
    "transactions.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6811c453",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert columns to lowercase for easier handling\n",
    "impressions.columns = impressions.columns.str.lower()\n",
    "products.columns = products.columns.str.lower()\n",
    "transactions.columns = transactions.columns.str.lower()\n",
    "\n",
    "# check for missing values\n",
    "print(impressions.isnull().sum())\n",
    "print(products.isnull().sum())\n",
    "print(transactions.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54132f82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10 missing values in transactions.revenue\n",
    "\n",
    "# merge transactions to product level to get total revenue and total quantity sold per product\n",
    "txns_to_products = transactions.merge(\n",
    "    products,\n",
    "    on=\"product_id\",\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "txns_to_products.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02f11210",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets first create synthetic data of product margin since profitability derives from margins not just revenue\n",
    "\n",
    "txns_to_products['merch_subcategory'].unique()\n",
    "\n",
    "# Define margin rules for each subcategory\n",
    "margin_rules = {\n",
    "    \"Fizzy Drinks\": (10, 20),\n",
    "    \"Nuts & Dried Fruit\": (20, 35),\n",
    "    \"Biscuits & Cookies\": (25, 40), # Brand-driven, impulse buys\n",
    "    \"Chocolate\": (35, 55),\n",
    "    \"Crisps\": (25, 40), # Brand-driven, impulse buys\n",
    "    \"Healthier Crisps\": (20, 35),\n",
    "    \"Sweets\": (35, 55),\n",
    "    \"Still Water\": (5, 12), # Commodity, low margin\n",
    "    \"Crackers & Crispbreads\": (20, 30),\n",
    "    \"Popcorn & Pretzels\": (25, 40),\n",
    "    \"Protein & Snack Bars\": (30, 50), # Niche, higher margin\n",
    "    \"Energy & Sports Drinks\": (25, 40),\n",
    "    \"Squashes & Cordials\": (15, 25), # Low-cost, low-margin\n",
    "    \"Juice & Smoothies\": (20, 35),\n",
    "    \"Mixers\": (15, 25),\n",
    "    \"Fruit Drinks\": (15, 25),\n",
    "    \"Gum & Mints\": (30, 50), # Impulse buys, higher margin\n",
    "    \"Iced Tea & Coffee\": (20, 35),\n",
    "    \"Sparkling Water\": (8, 15), # Commodity, low margin\n",
    "    \"Wellness & Protein Drinks\": (30, 50),\n",
    "    \"Meat Snacking\": (25, 40) # Niche, higher margin\n",
    "}\n",
    "\n",
    "def assign_margin(subcategory):\n",
    "    low, high = margin_rules.get(subcategory, (10, 25))  # default if unknown\n",
    "    return np.random.uniform(low, high)\n",
    "\n",
    "# Apply to your product catalog\n",
    "txns_to_products[\"margin\"] = txns_to_products[\"merch_subcategory\"].apply(assign_margin)\n",
    "txns_to_products\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0277ef4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "txns_to_products.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7251be07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now let us fill the revenue missing values based on quantity sold * price * (1 + margin%)\n",
    "txns_to_products['unit_price'] = txns_to_products['revenue'] + txns_to_products['total_discount'] / txns_to_products['quantity']\n",
    "\n",
    "subcat_price = (\n",
    "    txns_to_products.groupby('product_id')['unit_price']\n",
    "    .mean()\n",
    "    .reset_index()\n",
    "    .rename(columns={'unit_price': 'avg_product_price'})\n",
    ")\n",
    "\n",
    "txns_to_products = txns_to_products.merge(\n",
    "    subcat_price,\n",
    "    on='product_id',\n",
    "    how='left'\n",
    ")\n",
    "txns_to_products['revenue'] = txns_to_products.apply(\n",
    "    lambda row: row['quantity'] * row['avg_product_price'] * (1 + row['margin']/100) if pd.isnull(row['revenue']) else row['revenue'],\n",
    "    axis=1\n",
    ")   \n",
    "\n",
    "txns_to_products.head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43d89049",
   "metadata": {},
   "outputs": [],
   "source": [
    "txns_to_products.revenue.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "685b4fe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "txns_to_products.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "931eaeeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "txns_to_products.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc17067d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now merge impressions with products to get product details\n",
    "txns_imprs_products = (\n",
    "    txns_to_products.groupby(['user_id','product_id'])\n",
    "    .agg({\n",
    "        'created_at_local': 'first',\n",
    "        'location_id': 'first',\n",
    "        'is_newbie_order': 'first',\n",
    "        'is_fam': 'first',\n",
    "        'is_fam_exclusive_pricing_applied': 'first',\n",
    "        'is_on_promo': 'first',\n",
    "        'product_name': 'first',\n",
    "        'merch_category': 'first',\n",
    "        'merch_subcategory': 'first',\n",
    "        'brand': 'first',\n",
    "        \"quantity\": \"sum\",\n",
    "        \"revenue\": \"sum\",\n",
    "        \"total_discount\": \"sum\",\n",
    "        \"order_id\": \"nunique\",\n",
    "        \"margin\": \"mean\",\n",
    "        \"unit_price\": \"mean\",\n",
    "        \"avg_product_price\": \"mean\"\n",
    "    })\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "txns_imprs_products.head(50)\n",
    "\n",
    "txns_imprs_products = txns_imprs_products.merge(impressions, on=[\"user_id\", \"product_id\"], how=\"left\")\n",
    "\n",
    "txns_imprs_products.fillna({'n_impressions': 0, 'n_impressions_with_atc': 0}, inplace=True)\n",
    "txns_imprs_products\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "152b2b55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill missing unit_price or avg_product_price with fallback values\n",
    "txns_imprs_products[\"unit_price\"] = txns_imprs_products[\"unit_price\"].fillna(\n",
    "    txns_imprs_products[\"avg_product_price\"]\n",
    ")\n",
    "txns_imprs_products[\"avg_product_price\"].isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a77eb1ef",
   "metadata": {},
   "source": [
    "### User and Product Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3724fa7",
   "metadata": {},
   "source": [
    "#### Single Product Volume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6abce66",
   "metadata": {},
   "outputs": [],
   "source": [
    "most_bought_volume = txns_imprs_products.groupby('product_name')['quantity'].sum().nlargest(50)\n",
    "most_bought_volume\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cbea07c",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_products = txns_imprs_products.groupby('product_name')['quantity'].sum().sort_values(ascending=False).head(20)\n",
    "plt.figure(figsize=(12,6))\n",
    "sns.barplot(x=top_products.values, y=top_products.index)\n",
    "plt.title(\"Top 20 Products by Quantity Sold\")\n",
    "plt.xlabel(\"Total Quantity\")\n",
    "plt.ylabel(\"Product Name\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ba94b65",
   "metadata": {},
   "source": [
    "### Single Product by Revenue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c29708a",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_products = txns_imprs_products.groupby('product_name')['revenue'].sum().sort_values(ascending=False).head(20)\n",
    "plt.figure(figsize=(12,6))\n",
    "sns.barplot(x=top_products.values, y=top_products.index)\n",
    "plt.title(\"Top 20 Products by Revenue\")\n",
    "plt.xlabel(\"Total Revenue\")\n",
    "plt.ylabel(\"Product Name\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4a8f9a1",
   "metadata": {},
   "source": [
    "### Single product by total net margin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1222565d",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_products_margin = (\n",
    "    txns_imprs_products.groupby(\"product_name\")\n",
    "    .agg(total_margin=(\"margin\", \"sum\"),\n",
    "         total_revenue=(\"revenue\", \"sum\"),\n",
    "         total_quantity=(\"quantity\", \"sum\"))\n",
    "    .reset_index()\n",
    "    .sort_values(\"total_margin\", ascending=False)\n",
    ")\n",
    "\n",
    "print(\"🔝 Top 20 Products by Absolute Margin:\")\n",
    "display(top_products_margin.head(20))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b96dfda",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_products_margin = top_products_margin.head(20)\n",
    "plt.figure(figsize=(12,8))\n",
    "\n",
    "palette = sns.color_palette(\"rocket\", n_colors=top_products_margin.shape[0])\n",
    "\n",
    "sns.barplot(\n",
    "    x=\"total_margin\",\n",
    "    y=\"product_name\",\n",
    "    data=top_products_margin,\n",
    "    palette=palette\n",
    ")\n",
    "\n",
    "plt.title(\"Top 20 Products by Absolute Margin\", fontsize=16)\n",
    "plt.xlabel(\"Total Margin ($)\", fontsize=12)\n",
    "plt.ylabel(\"Product\", fontsize=12)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5494984",
   "metadata": {},
   "source": [
    "#### Porduct pairs Co-occurance - Most often sold together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed5f69bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import combinations\n",
    "from collections import Counter\n",
    "\n",
    "purchased = txns_imprs_products[txns_imprs_products['revenue'] > 0]\n",
    "\n",
    "order_products = purchased.groupby('order_id')['product_name'].apply(list)\n",
    "\n",
    "pair_counter = Counter()\n",
    "\n",
    "for products in order_products:\n",
    "    # Only consider baskets with 2+ products\n",
    "    if len(products) > 1:\n",
    "        pair_counter.update(combinations(sorted(products), 2))\n",
    "\n",
    "pair_counts = pd.DataFrame(\n",
    "    [(p1, p2, c) for (p1, p2), c in pair_counter.items()],\n",
    "    columns=['product_1','product_2','count']\n",
    ")\n",
    "\n",
    "pair_counts['product_pair'] = pair_counts['product_1'] + \" & \" + pair_counts['product_2']\n",
    "\n",
    "# Top 10\n",
    "top_pairs = pair_counts.sort_values(by='count', ascending=False).head(10)\n",
    "top_pairs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67ff7048",
   "metadata": {},
   "outputs": [],
   "source": [
    "palette = sns.color_palette(\"tab10\", len(top_pairs))\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(x='count', y='product_pair', data=top_pairs, palette=palette)\n",
    "plt.title(\"Top 10 Product Pairs Bought Together\")\n",
    "plt.xlabel(\"Number of Orders Buying Pair\")\n",
    "plt.ylabel(\"Product Pair\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3afee1ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recency, Frequency, Monetary (RMF) analysis\n",
    "# Recency: How recently a customer made a purchase\n",
    "# Frequency: How often they make a purchase\n",
    "# Monetary: How much money they spend\n",
    "\n",
    "txns_imprs_products['date'] = pd.to_datetime(txns_imprs_products['created_at_local'], dayfirst=True)\n",
    "snapshot_date = txns_imprs_products['date'].max() + pd.Timedelta(days=1)\n",
    "rfm = txns_imprs_products.groupby('user_id').agg(\n",
    "    recency=('date', lambda x: (snapshot_date - x.max()).days),\n",
    "    frequency=('product_name', 'count'),\n",
    "    monetary=('revenue', 'sum')\n",
    ").reset_index()\n",
    "\n",
    "# User-product matrix\n",
    "user_product = txns_imprs_products.pivot_table(index='user_id', columns='product_name', values='revenue', aggfunc='sum', fill_value=0)\n",
    "\n",
    "# Lift calculation for product pairs\n",
    "def compute_lift(df):\n",
    "    n_users = df.shape[0]\n",
    "    lifts = {}\n",
    "    products = df.columns\n",
    "    for i, p1 in enumerate(products):\n",
    "        for j, p2 in enumerate(products[i+1:], i+1):\n",
    "            p1_buy = (df[p1] > 0).sum()\n",
    "            p2_buy = (df[p2] > 0).sum()\n",
    "            both = ((df[p1] > 0) & (df[p2] > 0)).sum()\n",
    "            lift = (both / n_users) / ((p1_buy/n_users)*(p2_buy/n_users) + 1e-6)\n",
    "            lifts[(p1,p2)] = lift\n",
    "    return lifts\n",
    "\n",
    "lift_scores = compute_lift(user_product)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80213e65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualisations of RFM distributions of product pairs\n",
    "\n",
    "# Recency distribution\n",
    "plt.figure(figsize=(8,4))\n",
    "sns.histplot(rfm['recency'], bins=50, kde=True)\n",
    "plt.title(\"Distribution of Recency (days since last purchase)\")\n",
    "plt.xlabel(\"Days\")\n",
    "plt.ylabel(\"Number of Users\")\n",
    "plt.show()\n",
    "\n",
    "# Frequency distribution\n",
    "plt.figure(figsize=(8,4))\n",
    "sns.histplot(rfm['frequency'], bins=50, kde=True)\n",
    "plt.title(\"Distribution of Purchase Frequency\")\n",
    "plt.xlabel(\"Number of Orders\")\n",
    "plt.ylabel(\"Number of Users\")\n",
    "plt.show()\n",
    "\n",
    "# Monetary distribution\n",
    "plt.figure(figsize=(8,4))\n",
    "sns.histplot(rfm['monetary'], bins=50, kde=True)\n",
    "plt.title(\"Distribution of Monetary Value\")\n",
    "plt.xlabel(\"Total Revenue per User\")\n",
    "plt.ylabel(\"Number of Users\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d2b1171",
   "metadata": {},
   "outputs": [],
   "source": [
    "lift_df = pd.DataFrame([(k[0], k[1], v) for k,v in lift_scores.items()], columns=['product_1','product_2','lift'])\n",
    "\n",
    "# Filter top 20 products by revenue\n",
    "top_product_ids = txns_imprs_products['product_name'].value_counts().head(20).index\n",
    "lift_top = lift_df[(lift_df['product_1'].isin(top_product_ids)) & (lift_df['product_2'].isin(top_product_ids))]\n",
    "\n",
    "# Pivot Table\n",
    "lift_matrix = lift_top.pivot(index='product_1', columns='product_2', values='lift')\n",
    "\n",
    "plt.figure(figsize=(12,10))\n",
    "sns.heatmap(lift_matrix, annot=True, fmt=\".2f\", cmap='coolwarm')\n",
    "plt.title(\"Lift Between Top 20 Products\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8fc0969",
   "metadata": {},
   "source": [
    "### Show product pairs which when they are sold together and in dicount, yield the highest margin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96d9a425",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "purchased_small = purchased[['order_id', 'product_name', 'margin', 'total_discount']]\n",
    "\n",
    "# Group by order and convert to list of tuples (product, margin, discount)\n",
    "order_products = purchased_small.groupby('order_id').apply(\n",
    "    lambda df: list(zip(df['product_name'], df['margin'], df['total_discount']))\n",
    ")\n",
    "\n",
    "pair_margin_totals = defaultdict(float)\n",
    "pair_discount_totals = defaultdict(float)\n",
    "pair_counts = defaultdict(int)\n",
    "\n",
    "for products in order_products:\n",
    "    if len(products) > 1:\n",
    "        for (p1, m1, d1), (p2, m2, d2) in combinations(sorted(products), 2):\n",
    "            key = (p1, p2)\n",
    "            pair_margin_totals[key] += m1 + m2\n",
    "            pair_discount_totals[key] += d1 + d2\n",
    "            pair_counts[key] += 1\n",
    "\n",
    "bundle_analysis = pd.DataFrame([\n",
    "    (p1, p2, pair_margin_totals[(p1,p2)], pair_discount_totals[(p1,p2)], pair_counts[(p1,p2)])\n",
    "    for p1,p2 in pair_margin_totals.keys()\n",
    "], columns=['product_1','product_2','total_margin','total_discount','count'])\n",
    "\n",
    "bundle_analysis['product_pair'] = bundle_analysis['product_1'] + \" & \" + bundle_analysis['product_2']\n",
    "\n",
    "# Top 10 by total margin\n",
    "top_margin_pairs = bundle_analysis.sort_values('total_margin', ascending=False).head(10)\n",
    "top_margin_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1e82dc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "palette = sns.color_palette(\"Set2\", n_colors=top_margin_pairs.shape[0])\n",
    "\n",
    "plt.figure(figsize=(12,6))\n",
    "sns.barplot(\n",
    "    data=top_margin_pairs,\n",
    "    x='total_margin',\n",
    "    y='product_pair',\n",
    "    palette=palette\n",
    ")\n",
    "\n",
    "plt.title(\"Top 10 Discounted Product Pairs by Total Margin\", fontsize=16)\n",
    "plt.xlabel(\"Total Margin ($)\", fontsize=12)\n",
    "plt.ylabel(\"Product Pair\", fontsize=12)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d22e78f3",
   "metadata": {},
   "source": [
    "### Show top products by impression volume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "998fb9ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "top_impressions = txns_imprs_products.groupby(\"product_name\")[\"n_impressions\"].sum().sort_values(ascending=False).head(10)\n",
    "\n",
    "# top 10 products by impressions with atc\n",
    "top_impressions_atc = txns_imprs_products.groupby(\"product_name\")[\"n_impressions_with_atc\"].sum().sort_values(ascending=False).head(10)\n",
    "# Function to plot\n",
    "def plot_top10(series, title, color=\"skyblue\"):\n",
    "    plt.figure(figsize=(10,6))\n",
    "    sns.barplot(x=series.values, y=series.index, color=color)\n",
    "    plt.title(title)\n",
    "    plt.xlabel(\"Value\")\n",
    "    plt.ylabel(\"Product Name\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "plot_top10(top_impressions_atc, \"Top 10 Products by Impressions with ATC\", \"salmon\")\n",
    "plot_top10(top_impressions, \"Top 10 Products by Impressions\", \"plum\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb1a62a6",
   "metadata": {},
   "source": [
    "### Q1. Create initial features (that will maximise AOV) - Product Level and Marketing Level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b86c26e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "txns_imprs_products[\"margin_per_unit\"] = txns_imprs_products[\"margin\"] * txns_imprs_products[\"unit_price\"]\n",
    "txns_imprs_products[\"discount_pct\"] = txns_imprs_products[\"total_discount\"] / (txns_imprs_products[\"revenue\"] + txns_imprs_products[\"total_discount\"] + 1e-6)\n",
    "txns_imprs_products[\"atc_rate\"] = txns_imprs_products[\"n_impressions_with_atc\"] / txns_imprs_products[\"n_impressions\"]\n",
    "txns_imprs_products[\"conversion_rate\"] = txns_imprs_products[\"quantity\"] / (txns_imprs_products[\"n_impressions_with_atc\"] + 1e-6)\n",
    "txns_imprs_products[\"revenue_per_impression\"] = txns_imprs_products[\"revenue\"] / txns_imprs_products[\"n_impressions\"]\n",
    "txns_imprs_products[\"margin_per_impression\"] = txns_imprs_products[\"margin_per_unit\"] * txns_imprs_products[\"quantity\"] / (txns_imprs_products[\"n_impressions\"] + 1e-6)\n",
    "txns_imprs_products"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fa578c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "txns_imprs_products.to_csv(\"txns_imprs_products.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4f1c830",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "txns_imprs_products = pd.read_csv(\"txns_imprs_products.csv\")\n",
    "txns_imprs_products.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9e60e43",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import combinations\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "purchased = txns_imprs_products[txns_imprs_products['revenue'] > 0]\n",
    "\n",
    "# Step 1: Aggregate metrics per order\n",
    "order_metrics = purchased.groupby('order_id').agg({\n",
    "    'margin': 'sum',\n",
    "    'revenue': 'sum',\n",
    "    'quantity': 'sum'\n",
    "}).reset_index()\n",
    "\n",
    "# Step 2: Normalize metrics (0-1) using MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "order_metrics[['margin_norm','revenue_norm','quantity_norm']] = scaler.fit_transform(\n",
    "    order_metrics[['margin','revenue','quantity']]\n",
    ")\n",
    "\n",
    "# Step 3: Compute composite score (equal weighting)\n",
    "order_metrics['composite_score'] = (\n",
    "    order_metrics['margin_norm'] + order_metrics['revenue_norm'] + order_metrics['quantity_norm']\n",
    ")\n",
    "\n",
    "# Step 1: Use top 1000 orders by margin\n",
    "top_orders = order_metrics.nlargest(2000, 'composite_score')['order_id']\n",
    "df_top = purchased[purchased['order_id'].isin(top_orders)]\n",
    "df_top = df_top.groupby('order_id').head(2000).reset_index(drop=True)\n",
    "\n",
    "print(df_top.shape)\n",
    "# Step 2: Keep only needed columns\n",
    "\n",
    "# Step 1: Keep only needed columns in df_top\n",
    "cols = ['created_at_local', 'order_id', 'location_id', 'user_id', 'is_newbie_order',\n",
    "        'is_fam', 'product_id', 'quantity', 'revenue', 'is_fam_exclusive_pricing_applied',\n",
    "        'is_on_promo', 'total_discount', 'product_name', 'merch_category',\n",
    "        'merch_subcategory', 'brand', 'margin', 'unit_price', 'avg_product_price',\n",
    "        'n_impressions', 'n_impressions_with_atc', 'date', 'margin_per_unit',\n",
    "        'discount_pct', 'atc_rate', 'conversion_rate', 'revenue_per_impression',\n",
    "        'margin_per_impression']\n",
    "\n",
    "df_top = df_top[cols]\n",
    "\n",
    "# Step 2: Merge to create product pairs\n",
    "df_pairs = df_top.merge(df_top, on='order_id', suffixes=('_1', '_2'))\n",
    "\n",
    "# Step 3: Remove duplicate columns\n",
    "df_pairs = df_pairs.loc[:, ~df_pairs.columns.duplicated()]\n",
    "\n",
    "# Step 4: Keep only unique pairs\n",
    "df_pairs = df_pairs[df_pairs['product_name_1'] < df_pairs['product_name_2']]\n",
    "\n",
    "# Step 5: Compute bundle-level metrics\n",
    "df_pairs['bundle_total_margin'] = df_pairs['margin_1'] + df_pairs['margin_2']\n",
    "df_pairs['bundle_total_revenue'] = df_pairs['revenue_1'] + df_pairs['revenue_2']\n",
    "df_pairs['bundle_total_quantity'] = df_pairs['quantity_1'] + df_pairs['quantity_2']\n",
    "df_pairs['bundle_total_discount'] = df_pairs['total_discount_1'] + df_pairs['total_discount_2']\n",
    "df_pairs['bundle_total_impressions'] = df_pairs['n_impressions_1'] + df_pairs['n_impressions_2']\n",
    "df_pairs['bundle_total_add_to_cart'] = df_pairs['n_impressions_with_atc_1'] + df_pairs['n_impressions_with_atc_2']\n",
    "df_pairs['bundle_ctr'] = df_pairs['bundle_total_add_to_cart'] / (df_pairs['bundle_total_impressions'] + 1e-6)\n",
    "df_pairs['product_pair'] = df_pairs['product_name_1'] + \" & \" + df_pairs['product_name_2']\n",
    "df_pairs['bundle_conversion_rate'] = df_pairs['bundle_total_quantity'] / (df_pairs['bundle_total_add_to_cart'] + 1e-6)\n",
    "df_pairs['bundle_margin_per_unit'] = df_pairs['bundle_total_margin'] / (df_pairs['bundle_total_quantity'] + 1e-6)\n",
    "df_pairs['bundle_revenue_per_impression'] = df_pairs['bundle_total_revenue'] / (df_pairs['bundle_total_impressions'] + 1e-6)\n",
    "# Bundle AOV = total revenue / total quantity\n",
    "df_pairs['bundle_aov'] = df_pairs['bundle_total_revenue'] / (df_pairs['bundle_total_quantity'] + 1e-6)\n",
    "\n",
    "\n",
    "df_pairs.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53b747b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pairs.merch_subcategory_1.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d155b9ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: classify products into broader segments\n",
    "def classify_segment(row):\n",
    "    if row['merch_subcategory'] in ['Still Water', 'Sparkling Water', 'Juice & Smoothies',\n",
    "                                      'Nuts & Dried Fruit', 'Squashes & Cordials',\n",
    "                                      'Crackers & Crispbreads', 'Iced Tea & Coffee', 'Wellness & Protein Drinks',\n",
    "                                      'Fruit Drinks', 'Sparkling Water', 'Protein & Snack Bars',\n",
    "                                    'Mixers', 'Healthier Crisps']:\n",
    "        return 'Health-conscious'\n",
    "    elif row['merch_subcategory'] in ['Biscuits & Cookies', 'Fizzy Drinks', 'Crisps',\n",
    "                                       'Chocolate', 'Meat Snacking',\n",
    "                                       'Popcorn & Pretzels', 'Energy & Sports Drinks',\n",
    "                                       'Sweets', 'Gum & Mints']:\n",
    "        return 'Impulsive'\n",
    "    else:\n",
    "        return 'Other'\n",
    "\n",
    "def classify_bundle_segment(row):\n",
    "    seg1 = classify_segment({'merch_subcategory': row['merch_subcategory_1']})\n",
    "    seg2 = classify_segment({'merch_subcategory': row['merch_subcategory_2']})\n",
    "    \n",
    "    # If both products belong to the same segment, take that segment\n",
    "    if seg1 == seg2:\n",
    "        return seg1\n",
    "    else:\n",
    "        # Decide priority if mixed segments (e.g., Health-conscious > Impulsive)\n",
    "        if 'Health-conscious' in [seg1, seg2]:\n",
    "            return 'Health-conscious'\n",
    "        elif 'Impulsive' in [seg1, seg2]:\n",
    "            return 'Impulsive'\n",
    "        else:\n",
    "            return 'Other'\n",
    "\n",
    "df_pairs['bundle_segment'] = df_pairs.apply(classify_bundle_segment, axis=1)\n",
    "\n",
    "df_pairs.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0b44dac",
   "metadata": {},
   "source": [
    "## Q2. Creating Bundle Scoring /Ranking System to identify high performing bundles to increase AOV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec6c6fc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Average order value per product\n",
    "df_top['product_aov'] = df_top['revenue'] / (df_top['quantity'] + 1e-6)\n",
    "\n",
    "\n",
    "# Compute revenue, margin, high AOV, impressions per subcategory\n",
    "\n",
    "subcat_stats = df_top.groupby('merch_subcategory').agg({\n",
    "    'revenue': 'sum',\n",
    "    'margin': 'sum',\n",
    "    'n_impressions': 'sum',\n",
    "    'quantity': 'sum',\n",
    "    'product_aov': 'mean'\n",
    "}).reset_index()\n",
    "\n",
    "\n",
    "subcat_stats['revenue_norm'] = subcat_stats['revenue'] / subcat_stats['revenue'].sum()\n",
    "subcat_stats['margin_norm'] = subcat_stats['margin'] / subcat_stats['margin'].sum()\n",
    "subcat_stats['impression_norm'] = subcat_stats['n_impressions'] / subcat_stats['n_impressions'].sum()\n",
    "subcat_stats['product_aov_norm'] = subcat_stats['product_aov'] / subcat_stats['product_aov'].max()\n",
    "\n",
    "df_pairs = df_pairs.merge(\n",
    "    subcat_stats[['merch_subcategory','revenue_norm','margin_norm','impression_norm', 'product_aov_norm']].rename(\n",
    "        columns={\n",
    "            'revenue_norm':'revenue_norm_1',\n",
    "            'margin_norm':'margin_norm_1',\n",
    "            'impression_norm':'impression_norm_1',\n",
    "            'product_aov_norm':'product_aov_norm_1'\n",
    "        }\n",
    "    ), left_on='merch_subcategory_1', right_on='merch_subcategory', how='left'\n",
    ").drop(columns=['merch_subcategory'])\n",
    "\n",
    "df_pairs = df_pairs.merge(\n",
    "    subcat_stats[['merch_subcategory','revenue_norm','margin_norm','impression_norm', 'product_aov_norm']].rename(\n",
    "        columns={\n",
    "            'revenue_norm':'revenue_norm_2',\n",
    "            'margin_norm':'margin_norm_2',\n",
    "            'impression_norm':'impression_norm_2',\n",
    "            'product_aov_norm':'product_aov_norm_2'\n",
    "        }\n",
    "    ), left_on='merch_subcategory_2', right_on='merch_subcategory', how='left'\n",
    ").drop(columns=['merch_subcategory'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f6ee6dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pairs.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aa6346c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pairs[['revenue_norm','quantity_norm','margin_norm', 'impression_norm']] = scaler.fit_transform(\n",
    "    df_pairs[['bundle_total_revenue','bundle_total_quantity','bundle_total_margin', 'bundle_total_impressions']]\n",
    ")\n",
    "\n",
    "# MinMax scale AOV separately if you want it in scoring\n",
    "df_pairs['bundle_aov_norm'] = MinMaxScaler().fit_transform(df_pairs[['bundle_aov']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba5ce2cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Give weights\n",
    "df_pairs['bundle_score'] = (\n",
    "    0.3 * df_pairs['revenue_norm'] +\n",
    "    0.3 * df_pairs['margin_norm'] +\n",
    "    0.2 * df_pairs['quantity_norm'] +\n",
    "    0.2 * df_pairs['bundle_aov_norm']\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c1492dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_bundles = df_pairs.sort_values('bundle_score', ascending=False)\n",
    "top_bundles[['product_pair', 'bundle_segment', 'bundle_total_revenue', 'bundle_total_margin', 'bundle_total_quantity', 'bundle_total_impressions', 'bundle_score', 'bundle_aov']].head(5)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eb10c93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_pairs.to_csv(\"product_pair_construction.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa88d606",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df_pairs = pd.read_csv(\"product_pair_construction.csv\")\n",
    "df_pairs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30cce450",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "top_bundles = df_pairs.sort_values('bundle_score', ascending=False)\n",
    "top_bundles[['product_pair', 'bundle_segment', 'bundle_total_revenue', 'bundle_total_margin', 'bundle_total_quantity', 'bundle_total_impressions', 'bundle_score', 'bundle_aov']].head(50)\n",
    "\n",
    "\n",
    "top_bundles_plot = top_bundles.head(40)\n",
    "plt.figure(figsize=(10,6))\n",
    "sns.barplot(x='bundle_score', y='product_pair', data=top_bundles_plot, hue='bundle_segment', dodge=False)\n",
    "plt.title('Top 40 Bundles Combos by Score')\n",
    "plt.xlabel('Composite Bundle Score')\n",
    "plt.ylabel('Bundle')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95415287",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_bundles_plot[['product_pair','revenue_norm','margin_norm','quantity_norm']].set_index('product_pair').plot(kind='bar', stacked=True, figsize=(12,6))\n",
    "plt.title('Contribution of Revenue, Margin, Quantity to Bundle Score')\n",
    "plt.ylabel('Normalized Value')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db813411",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_bundles_per_segment = df_pairs.groupby('bundle_segment').apply(\n",
    "    lambda x: x.sort_values('bundle_score', ascending=False).head(20)\n",
    ").reset_index(drop=True)\n",
    "\n",
    "g = sns.FacetGrid(top_bundles_per_segment, col='bundle_segment', height=5, sharex=False)\n",
    "g.map_dataframe(sns.barplot, x='bundle_score', y='product_pair', palette='Set3', dodge=False)\n",
    "g.set_titles(col_template=\"{col_name} Segment\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "481cf079",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_conv_bundles = df_pairs.sort_values('bundle_conversion_rate', ascending=False).head(40)\n",
    "\n",
    "plt.figure(figsize=(10,6))\n",
    "sns.barplot(x='bundle_conversion_rate', y='product_pair', data=top_conv_bundles, palette='coolwarm')\n",
    "plt.xlabel('Bundle Conversion Rate (Add-to-Cart / Impressions)')\n",
    "plt.ylabel('Bundle')\n",
    "plt.title('Top 20 Bundles by Conversion Rate')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f67dbc2b",
   "metadata": {},
   "source": [
    "## Find bundles which perform well in terms of revenue and margin but are not discounted and drive them for promotion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b4ac079",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "df_pairs['bundle_name'] = (\n",
    "    df_pairs['product_name_1'] + \" + \" + df_pairs['product_name_2']\n",
    ")\n",
    "\n",
    "df_no_promo = df_pairs[(df_pairs['is_on_promo_1'] == False) & (df_pairs['is_on_promo_2'] == False)].copy()\n",
    "\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "df_no_promo[['rev_scaled','margin_scaled', 'quantity_scaled']] = scaler.fit_transform(\n",
    "    df_no_promo[['bundle_total_revenue','bundle_total_margin', 'bundle_total_quantity']]\n",
    ")\n",
    "\n",
    "df_no_promo['bundle_aov_scaled'] = MinMaxScaler().fit_transform(df_no_promo[['bundle_aov']])\n",
    "\n",
    "# Promotion Potential = equal weight of revenue + margin\n",
    "df_no_promo['promo_potential'] = (\n",
    "    0.3 * df_no_promo['rev_scaled'] +\n",
    "    0.3 * df_no_promo['margin_scaled'] +\n",
    "    0.2 * df_no_promo['quantity_scaled'] +\n",
    "    0.2 * df_no_promo['bundle_aov_scaled']\n",
    ")\n",
    "\n",
    "# Ranking bundles by promo potential\n",
    "top_promo_candidates = df_no_promo.sort_values('promo_potential', ascending=False)\n",
    "top_promo_candidates[['bundle_name',\n",
    "                      'bundle_total_revenue',\n",
    "                      'bundle_total_margin',\n",
    "                      'bundle_conversion_rate',\n",
    "                      'bundle_segment',\n",
    "                      'bundle_aov',\n",
    "                      'promo_potential']]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cd6385d",
   "metadata": {},
   "source": [
    "## Q3a. Calculate potential impact the promo candidates will bring to Gopuff\n",
    "\n",
    "### Steps to Estimate AOV Uplift\n",
    "\n",
    "- Current AOV (baseline):\n",
    "\n",
    "Already we have bundle_aov (or recomputed from bundle_total_revenue / bundle_total_quantity).\n",
    "\n",
    "- Promotion effect assumption:\n",
    "\n",
    "A discount (say 10–20%) increases conversion rate by some lift factor (e.g., +20–30%).\n",
    "\n",
    "More conversions → higher revenue & margin overall (despite lower unit price)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb77dff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_discount_per_bundle = df_pairs.groupby('product_pair')[['total_discount_1', 'total_discount_2']].max().reset_index()\n",
    "\n",
    "max_discount_per_bundle.rename(columns={'total_discount_1': 'max_discount_1', 'total_discount_2': 'max_discount_2'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d517e321",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_promo_candidates = top_promo_candidates.merge(\n",
    "    max_discount_per_bundle[['product_pair', 'max_discount_1', 'max_discount_2']],\n",
    "    on='product_pair',\n",
    "    how='left'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "299b5c0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "top_promo_candidates = top_promo_candidates[(top_promo_candidates['max_discount_1'] > 0.0) | (top_promo_candidates['max_discount_2'] > 0.0)]\n",
    "top_promo_candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "476bc32e",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_promo_candidates['discount_pct_assumed'] = top_promo_candidates[['max_discount_1', 'max_discount_2']].max(axis=1)\n",
    "\n",
    "top_promo_candidates['discount_pct_assumed'] = top_promo_candidates['discount_pct_assumed']\n",
    "\n",
    "# Make sure these columns are numeric\n",
    "df_pairs['bundle_conversion_rate'] = pd.to_numeric(df_pairs['bundle_conversion_rate'], errors='coerce')\n",
    "df_pairs['discount_pct_1'] = pd.to_numeric(df_pairs['discount_pct_1'], errors='coerce')\n",
    "df_pairs['discount_pct_2'] = pd.to_numeric(df_pairs['discount_pct_2'], errors='coerce')\n",
    "\n",
    "df_pairs['is_promo'] = (\n",
    "    (df_pairs['discount_pct_1'] > 0) | \n",
    "    (df_pairs['discount_pct_2'] > 0) |\n",
    "    (df_pairs['is_on_promo_1'] == 1) | \n",
    "    (df_pairs['is_on_promo_2'] == 1)\n",
    ")\n",
    "\n",
    "# Baseline (no promo)\n",
    "baseline = df_pairs[df_pairs['is_promo'] == False].groupby('bundle_segment')['bundle_conversion_rate'].mean()\n",
    "\n",
    "# Promo (with discount or promo flag)\n",
    "promo = df_pairs[df_pairs['is_promo'] == True].groupby('bundle_segment')['bundle_conversion_rate'].mean()\n",
    "\n",
    "conv_rate_uplift = ((promo / baseline) - 1).fillna(0)\n",
    "\n",
    "conv_rate_uplift_map = {\n",
    "    'Health-conscious': 0.699318,\n",
    "    'Impulsive': 0.590862\n",
    "}\n",
    "\n",
    "top_promo_candidates['conv_rate_uplift'] = top_promo_candidates['bundle_segment'].map(conv_rate_uplift_map)\n",
    "\n",
    "top_promo_candidates['expected_bundle_quantity'] = (\n",
    "    top_promo_candidates['bundle_total_quantity'] * (1 + top_promo_candidates['conv_rate_uplift']))\n",
    "\n",
    "top_promo_candidates['expected_bundle_conversion_rate'] = (\n",
    "    top_promo_candidates['bundle_conversion_rate'] * (1 + top_promo_candidates['conv_rate_uplift']))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32968ef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Revenue impact: lower price per unit but more units sold\n",
    "top_promo_candidates['expected_bundle_revenue'] = (\n",
    "    top_promo_candidates['bundle_total_revenue'] \n",
    "    * (1 - top_promo_candidates['discount_pct_assumed']) \n",
    "    * (1 + top_promo_candidates['conv_rate_uplift'])\n",
    ")\n",
    "\n",
    "# Margin impact: lower margin per unit but more units sold\n",
    "\n",
    "top_promo_candidates['expected_bundle_margin'] = (\n",
    "    top_promo_candidates['bundle_total_margin'] \n",
    "    * (1 - top_promo_candidates['discount_pct_assumed']) \n",
    "    * (1 + top_promo_candidates['conv_rate_uplift'])\n",
    ")\n",
    "# Revenue uplift %\n",
    "\n",
    "top_promo_candidates['revenue_uplift_pct'] = (\n",
    "    (top_promo_candidates['expected_bundle_revenue'] - top_promo_candidates['bundle_total_revenue'])\n",
    "    / top_promo_candidates['bundle_total_revenue'] * 100\n",
    ")\n",
    "\n",
    "top_promo_candidates['margin_uplift_pct'] = (\n",
    "    (top_promo_candidates['expected_bundle_margin'] - top_promo_candidates['bundle_total_margin'])\n",
    "    / top_promo_candidates['bundle_total_margin'] * 100\n",
    ")\n",
    "\n",
    "# Expected AOV\n",
    "top_promo_candidates['expected_bundle_aov'] = (\n",
    "    top_promo_candidates['expected_bundle_revenue'] / (top_promo_candidates['expected_bundle_quantity'])\n",
    ")\n",
    "\n",
    "top_promo_candidates['expected_aov_uplift'] =  top_promo_candidates['expected_bundle_revenue'] / top_promo_candidates['expected_bundle_quantity']\n",
    "\n",
    "# Expected AOV uplift %\n",
    "top_promo_candidates['aov_uplift_pct'] = (\n",
    "    (top_promo_candidates['expected_bundle_aov'] - top_promo_candidates['bundle_aov']) \n",
    "    / (top_promo_candidates['bundle_aov'] + 1e-6))  # avoid division by zero\n",
    "\n",
    "\n",
    "\n",
    "promo_projection = top_promo_candidates.sort_values('revenue_uplift_pct', ascending=False)\n",
    "promo_projection = promo_projection[['bundle_name', \n",
    "                             'bundle_total_quantity','expected_bundle_quantity',\n",
    "                             'bundle_total_revenue','expected_bundle_revenue','revenue_uplift_pct',\n",
    "                             'bundle_total_margin','expected_bundle_margin','margin_uplift_pct',\n",
    "                             'bundle_aov','expected_bundle_aov', 'aov_uplift_pct']]\n",
    "promo_projection.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0961f794",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "top_expected_bundles = promo_projection.sort_values('expected_bundle_revenue', ascending=False).head(50)\n",
    "\n",
    "plt.figure(figsize=(20, 15))\n",
    "sns.barplot(x='expected_bundle_revenue', y='bundle_name', data=top_expected_bundles, palette='flare')\n",
    "plt.xlabel(\"Expected Revenue (£)\")\n",
    "plt.ylabel(\"Bundle\")\n",
    "plt.title(\"Top 50 Bundles for Promotion by Expected Revenue\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73dcbf8e",
   "metadata": {},
   "source": [
    "## Q3b. Build a framework to assess if the proposed promo bundles will be profitable for Gopuff\n",
    "\n",
    "#### Objective: Increase Average Order Value (AOV), revenue, and margin through strategic product bundle promotions (which are not in promo right now but have lots of potential) in high-frequency categories (Snacks, Drinks, Confectionery).\n",
    "\n",
    "#### Hypothesis: Promoting top-performing bundles with minimal current discount will encourage higher order quantities and incremental revenue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec4d4ecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Impact Quantification \n",
    "\n",
    "total_revenue_before = top_promo_candidates['bundle_total_revenue'].sum()\n",
    "total_revenue_after = top_promo_candidates['expected_bundle_revenue'].sum()\n",
    "revenue_uplift_percent = (total_revenue_after / total_revenue_before - 1) \n",
    "\n",
    "total_margin_before = top_promo_candidates['bundle_total_margin'].sum()\n",
    "total_margin_after = top_promo_candidates['expected_bundle_margin'].sum()\n",
    "margin_uplift_percent = (total_margin_after / total_margin_before - 1) \n",
    "\n",
    "margin_pct_before = total_margin_before / total_revenue_before\n",
    "margin_pct_after = total_margin_after / total_revenue_after\n",
    "\n",
    "print(f\"Total Revenue Before Promotion: £{total_revenue_before:,.2f}\")\n",
    "print(f\"Total Revenue After Promotion: £{total_revenue_after:,.2f}\")\n",
    "print(f\"Total Margin Before Promotion: £{total_margin_before:,.2f}\")\n",
    "print(f\"Total Margin After Promotion: £{total_margin_after:,.2f}\")\n",
    "print(f\"Projected Revenue Uplift: {revenue_uplift_percent:.2f}%\")\n",
    "print(f\"Projected Margin Uplift: {margin_uplift_percent:.2f}%\")\n",
    "print(f\"Margin Percentage Before Promotion: {margin_pct_before:.2f}%\")\n",
    "print(f\"Margin Percentage After Promotion: {margin_pct_after:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c4ed824",
   "metadata": {},
   "source": [
    "### A/B Testing Framework to measure if the projected assumptions will have a quantifiable impact at Gopuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14f4d6bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check which features to include in a/b testing\n",
    "\n",
    "metrics = ['bundle_aov', 'bundle_total_revenue', 'bundle_total_margin', 'bundle_conversion_rate']\n",
    "\n",
    "# Features to test\n",
    "features = ['location_id_1', 'is_fam_1', 'is_newbie_order_1', 'bundle_segment']\n",
    "\n",
    "for f in features:\n",
    "    print(f\"Feature: {f}\")\n",
    "    print(df_pairs.groupby(f)[metrics].mean())\n",
    "    print(\"-\"*50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91d8edfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import ttest_ind, f_oneway\n",
    "\n",
    "# Define columns to check\n",
    "boolean_cols = ['is_fam_1', 'is_fam_2', 'is_newbie_order_1', 'is_newbie_order_2', 'is_on_promo_1', 'is_on_promo_2']\n",
    "categorical_cols = ['location_id_1', 'location_id_2', 'bundle_segment']\n",
    "\n",
    "results = []\n",
    "\n",
    "# Boolean columns: t-test\n",
    "for col in boolean_cols:\n",
    "    if col in top_promo_candidates.columns:\n",
    "        group0 = top_promo_candidates[top_promo_candidates[col]==0]['bundle_aov']\n",
    "        group1 = top_promo_candidates[top_promo_candidates[col]==1]['bundle_aov']\n",
    "        t_stat, p_val = ttest_ind(group0, group1, equal_var=False)\n",
    "        results.append({\n",
    "            'feature': col,\n",
    "            'type': 'boolean',\n",
    "            't_stat': t_stat,\n",
    "            'p_value': p_val\n",
    "        })\n",
    "\n",
    "# Categorical columns: ANOVA\n",
    "for col in categorical_cols:\n",
    "    if col in top_promo_candidates.columns:\n",
    "        groups = [top_promo_candidates[top_promo_candidates[col]==cat]['bundle_aov'] for cat in top_promo_candidates[col].dropna().unique()]\n",
    "        f_stat, p_val = f_oneway(*groups)\n",
    "        results.append({\n",
    "            'feature': col,\n",
    "            'type': 'categorical',\n",
    "            'f_stat': f_stat,\n",
    "            'p_value': p_val\n",
    "        })\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df.sort_values('p_value', inplace=True)\n",
    "print(results_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68935d7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_promo_candidates.to_csv(\"top_promo_candidates.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01c91756",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_promo_candidates = pd.read_csv(\"top_promo_candidates.csv\")\n",
    "top_promo_candidates.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "259ee223",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# =============================\n",
    "# 0️⃣ Select top 20 test bundles based on performance\n",
    "# =============================\n",
    "test_candidates = top_promo_candidates\n",
    "# Step 1: Drop duplicate product pairs so each pair appears only once\n",
    "test_candidates = test_candidates.drop_duplicates(subset='product_pair')\n",
    "\n",
    "# Step 2: Pick the top 20 based on expected_bundle_aov\n",
    "test_top20 = test_candidates.nlargest(20, 'expected_bundle_aov')\n",
    "print(\"Selected Test Bundles:\")\n",
    "test_top20[['bundle_name', 'expected_bundle_aov', 'expected_bundle_revenue', 'expected_bundle_margin']]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "167a20e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================\n",
    "# 1️⃣ Select top 20 control bundles ensuring different product pairs, only in selected locations\n",
    "# =============================\n",
    "\n",
    "cols_needed = [\n",
    "    'product_pair', 'bundle_segment', 'location_id_1', 'is_fam_2',\n",
    "    'bundle_total_revenue', 'bundle_total_margin', 'bundle_total_quantity',\n",
    "    'bundle_aov', 'bundle_margin_per_unit', 'bundle_conversion_rate', 'bundle_ctr'\n",
    "]\n",
    "\n",
    "control_candidates = df_pairs[cols_needed]\n",
    "\n",
    "# Remove product pairs already in test\n",
    "control_candidates = control_candidates[~control_candidates['product_pair'].isin(test_top20['product_pair'])]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd383c32",
   "metadata": {},
   "outputs": [],
   "source": [
    "control_candidates['performance_score'] = (\n",
    "    control_candidates['bundle_total_revenue'].rank(ascending=False) +\n",
    "    control_candidates['bundle_total_margin'].rank(ascending=False) +\n",
    "    control_candidates['bundle_total_quantity'].rank(ascending=False) +\n",
    "    control_candidates['bundle_conversion_rate'].rank(ascending=False) +\n",
    "    control_candidates['bundle_aov'].rank(ascending=False)\n",
    ")\n",
    "\n",
    "control_top20 = control_candidates.nlargest(20, 'performance_score')\n",
    "\n",
    "print(\"Selected Control Bundles:\")\n",
    "control_top20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "384ba02d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================\n",
    "# 2️⃣ Prepare aggregated long table for test\n",
    "# =============================\n",
    "test_long = test_top20.groupby([\n",
    "    'product_pair', 'bundle_segment', 'location_id_1', 'is_fam_2'\n",
    "]).agg(\n",
    "    total_revenue=('bundle_total_revenue', 'sum'),\n",
    "    total_margin=('bundle_total_margin', 'sum'),\n",
    "    total_quantity=('bundle_total_quantity', 'sum'),\n",
    "    avg_bundle_aov=('bundle_aov', 'mean'),\n",
    "    avg_bundle_margin_per_unit=('bundle_margin_per_unit', 'mean'),\n",
    "    conversion_rate=('bundle_conversion_rate', 'mean'),\n",
    "    ctr=('bundle_ctr', 'mean'),\n",
    "    expected_bundle_aov=('expected_bundle_aov', 'mean'),\n",
    "    expected_bundle_margin=('expected_bundle_margin', 'sum'),\n",
    "    expected_bundle_revenue=('expected_bundle_revenue', 'sum'),\n",
    "    expected_bundle_quantity=('expected_bundle_quantity', 'sum'),\n",
    "    expected_aov_uplift=('expected_aov_uplift', 'mean')\n",
    ").reset_index()\n",
    "test_long['arm'] = 'Test'\n",
    "\n",
    "# =============================\n",
    "# 3️⃣ Prepare aggregated long table for control\n",
    "# =============================\n",
    "control_long = control_top20.groupby([\n",
    "    'product_pair', 'bundle_segment', 'location_id_1', 'is_fam_2'\n",
    "]).agg(\n",
    "    total_revenue=('bundle_total_revenue', 'sum'),\n",
    "    total_margin=('bundle_total_margin', 'sum'),\n",
    "    total_quantity=('bundle_total_quantity', 'sum'),\n",
    "    avg_bundle_aov=('bundle_aov', 'mean'),\n",
    "    avg_bundle_margin_per_unit=('bundle_margin_per_unit', 'mean'),\n",
    "    conversion_rate=('bundle_conversion_rate', 'mean'),\n",
    "    ctr=('bundle_ctr', 'mean')\n",
    ").reset_index()\n",
    "control_long['arm'] = 'Control'\n",
    "\n",
    "# =============================\n",
    "# 4️⃣ Combine test and control\n",
    "# =============================\n",
    "ab_long_table = pd.concat([control_long, test_long], axis=0, ignore_index=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7706bec",
   "metadata": {},
   "outputs": [],
   "source": [
    "markdown_table = ab_long_table.to_markdown(index=False)\n",
    "print(markdown_table)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1297ceff",
   "metadata": {},
   "source": [
    "### The control vs test product pairs are different for the following reasons: \n",
    "\n",
    "### - Avoid contamination\n",
    "\n",
    "If the same product pair appears in both test and control:\n",
    "\n",
    "You can’t tell whether changes in sales or conversion are due to the promotion or just natural product performance.\n",
    "\n",
    "Overlapping products “contaminate” the control, making it harder to isolate the true impact.\n",
    "\n",
    "### - Preserve the “never-promoted” property\n",
    "\n",
    "The test bundles are products that have never been promoted.\n",
    "\n",
    "If we allowed the same product pairs in control, we’d risk including products that have historical promotion data, which breaks the test assumption that you’re seeing the first-time effect.\n",
    "\n",
    "### - Avoid cross-influence\n",
    "\n",
    "If a product in the test group has previously appeared in control bundles, past promotion effects could carry over.\n",
    "\n",
    "Different pairs ensure the uplift is due to the new promotion, not historical effects or overlapping exposure.\n",
    "\n",
    "### - Maximize learning from the test\n",
    "\n",
    "Using novel bundles in test means we can discover which products actually perform well when promoted.\n",
    "\n",
    "If test and control overlap, we lose the ability to learn which untested bundles have potential.\n",
    "\n",
    "### - Cleaner metrics\n",
    "\n",
    "Revenue, margin, and conversion improvements are easier to attribute to the promotion.\n",
    "\n",
    "Non-overlapping pairs reduce noise from products that might already have optimized pricing, bundling, or marketing exposure.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aabe56a",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_candidates.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00da39a7",
   "metadata": {},
   "source": [
    "## Q4. Roll out plan for Gopuff's success and next steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f81608c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Impact Quantification \n",
    "\n",
    "# ✅ Compute baseline (control) averages\n",
    "baseline_revenue = control_candidates['bundle_total_revenue'].mean()\n",
    "baseline_margin = control_candidates['bundle_total_margin'].mean()\n",
    "\n",
    "# ✅ Compute test averages\n",
    "test_revenue = test_candidates['expected_bundle_revenue'].mean()\n",
    "test_margin = test_candidates['expected_bundle_margin'].mean()\n",
    "\n",
    "# Compute uplifts\n",
    "revenue_uplift_percent = (test_revenue / baseline_revenue - 1) * 100\n",
    "margin_uplift_percent = (test_margin / baseline_margin - 1) * 100\n",
    "\n",
    "print(f\"Baseline Revenue per Bundle: £{baseline_revenue:.2f}\")\n",
    "print(f\"Test Revenue per Bundle: £{test_revenue:.2f}\")\n",
    "print(f\"Revenue Uplift: {revenue_uplift_percent:.2f}%\")\n",
    "\n",
    "print(f\"Baseline Margin per Bundle: £{baseline_margin:.2f}\")\n",
    "print(f\"Test Margin per Bundle: £{test_margin:.2f}\")\n",
    "print(f\"Margin Uplift: {margin_uplift_percent:.2f}%\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a38055a9",
   "metadata": {},
   "source": [
    "# Rollout Plan for Bundle Promotions\n",
    "\n",
    "## 1. Test & Control Design\n",
    "- **Test Bundles (20)**  \n",
    "  - Product pairs never promoted before.  \n",
    "  - Selected based on high potential (co-purchase frequency, high margin, strong revenue baseline).  \n",
    "- **Control Bundles (20)**  \n",
    "  - Different product pairs from test set.  \n",
    "  - Include products that may have been promoted in the past.  \n",
    "  - Provide a realistic business-as-usual benchmark.  \n",
    "\n",
    "- **Markets & Segments**  \n",
    "  - All selected markets (e.g., 12 pilot markets).  \n",
    "  - Run across **2 customer segments**:  \n",
    "    - Segment A: Price-sensitive (e.g., students).  \n",
    "    - Segment B: Value-driven (e.g., families).  \n",
    "\n",
    "---\n",
    "\n",
    "## 2. Quantified Impact (Illustrative Example)\n",
    "Using test/control A/B methodology, impact is quantified as:  \n",
    "\n",
    "\\[\n",
    "\\text{Revenue Uplift \\%} = \\frac{\\text{Revenue(Test)}}{\\text{Revenue(Control)}} - 1\n",
    "\\]\n",
    "\n",
    "\\[\n",
    "\\text{Margin Uplift \\%} = \\frac{\\text{Margin(Test)}}{\\text{Margin(Control)}} - 1\n",
    "\\]\n",
    "\n",
    "- **Baseline (Control):**  \n",
    "  - Avg Revenue per Bundle = **£8.43**  \n",
    "  - Avg Margin per Bundle = **£58.30**  \n",
    "\n",
    "- **Test (Never Promoted Bundles):**  \n",
    "  - Avg Revenue per Bundle = **£22.76**  \n",
    "  - Avg Margin per Bundle = **£85.31**  \n",
    "\n",
    "- **Impact and Strategic Benefits:**  \n",
    "  - **+169.78% Revenue Uplift** 🚀  \n",
    "  - **+46.32% Margin Uplift** 💰  \n",
    "  - **Higher AOV:** customers spend significantly more per basket when new bundles are introduced.  \n",
    "  - **Competitive Advantage:** Unlocks incremental revenue streams by targeting segments that were previously under-monetized.  \n",
    "  - **Customer Retention and higher LTV:** New bundles attract repeat purchases and improve stickiness.  \n",
    "- **Incremental Customers:** attract new customers who see value in fresh bundles.  \n",
    "- **Reduced Churn:** higher perceived value keeps customers loyal. \n",
    "- **Operational Learnings:** framework for identifying future profitable bundles.  \n",
    "\n",
    "---\n",
    "\n",
    "## 3. Rollout & Scaling Plan\n",
    "\n",
    "### Phase 1 – Pilot\n",
    "- Run A/B test in **12 markets × 2 segments**.  \n",
    "- Collect results over **6 weeks** to capture seasonality.  \n",
    "- Metrics: Revenue uplift, Margin uplift, AOV increase, customer retention.  \n",
    "\n",
    "### Phase 2 – Quantified Expansion\n",
    "- Scale winning bundles to **top 50 markets and more segments and audiences**.  \n",
    "- Integrate bundles into seasonal promotions calendar.  \n",
    "- Monitor KPIs weekly with real-time dashboards.  \n",
    "\n",
    "### Phase 3 – ML-Driven Optimization\n",
    "- Build **bundle scoring algorithm**:  \n",
    "  - Inputs: margin, revenue, co-purchase lift, discount elasticity, promo history, conversion rate, segments, location, recency, frequency, monetary value, demographics, etc.  \n",
    "  - Outputs: predicted uplift (AOV, margin). --> Regressor Algorithms\n",
    "- Use **multi-armed bandits** to dynamically allocate bundle discounts.  \n",
    "- Continuously refresh bundle portfolio every 2–4 weeks.  \n",
    "\n",
    "### Phase 4 – Full Rollout & Automation\n",
    "- Nationwide rollout of **validated bundles**.  \n",
    "- Automated monitoring of:  \n",
    "  - Net Revenue Impact  \n",
    "  - Net Margin Impact  \n",
    "  - Promo ROI  \n",
    "  - Retention / New Customers Gained  \n",
    "- Sunset underperforming bundles quickly.  \n",
    "\n",
    "---\n",
    "\n",
    "## 4. Executive Summary\n",
    "By launching **20 new test bundles** against **20 controls across all markets and 2 customer segments**, we can:  \n",
    "- Deliver **~40% uplift in revenue and margin per bundle**.  \n",
    "- Drive **higher AOV and customer loyalty**.  \n",
    "- Build a **repeatable ML-driven framework** to identify and scale bundles profitably.  \n",
    "- Secure **competitive advantage** through differentiated promotions strategy.  \n",
    "\n",
    "This phased rollout ensures we **quantify impact, scale efficiently, and automate with ML** for long-term growth.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53a14010",
   "metadata": {},
   "source": [
    "## Next steps small ML Script with Kedro Framework as pipeline wrapper"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6458c37d",
   "metadata": {},
   "source": [
    "## 🔮 Next Steps: Full-Stack ML for Bundle Optimization\n",
    "\n",
    "To demonstrate **ML & DL capabilities**, I created a **small prototype script** that predicts **expected bundle AOV** using a `GradientBoostingRegressor` trained on bundle-level features (e.g., total revenue, margin, quantity, CTR, conversion rate, discount, etc.).  \n",
    "\n",
    "This quick script is a proof-of-concept, but to **deploy a production-ready ML system**, we need to implement the **full machine learning workflow**, which includes:  \n",
    "\n",
    "### 1. 🔧 Data Preparation & Feature Engineering\n",
    "- Create new engineered features (e.g., **discount elasticity, margin per impression, revenue per unit, uplift signals**).  \n",
    "- Aggregate user and location-level signals (e.g., **seasonality, user cohort, demand shifts**).  \n",
    "- Normalize / scale features where appropriate.  \n",
    "\n",
    "### 2. 📊 Exploratory Data Analysis (EDA) & Statistics\n",
    "- Detect and handle **outliers** using IQR or robust z-scores.  \n",
    "- Conduct **distribution analysis** to ensure stable feature ranges.  \n",
    "- Use **correlation matrices & VIF (Variance Inflation Factor)** to check for **multicollinearity** between features.  \n",
    "\n",
    "### 3. 🧮 Feature Selection & Encoding\n",
    "- Apply **feature importance analysis** (e.g., SHAP, permutation importance) to prioritize predictive drivers.  \n",
    "- Use **One-Hot Encoding or Label Encoding** for categorical features like `bundle_segment` or `brand` or `merch_subcategory`.  \n",
    "- Drop redundant or low-signal features to avoid noise.  \n",
    "\n",
    "### 4. 🏗️ Model Development\n",
    "- Train multiple models (Gradient Boosting, XGBoost, Random Forests, Regularized Linear Models).  \n",
    "- Apply **hyperparameter tuning** (Grid Search / Random Search / Bayesian Optimization).  \n",
    "- Validate performance using **cross-validation** across markets & segments.  \n",
    "\n",
    "### 5. 📈 Evaluation Metrics\n",
    "- **R² / RMSE** for predictive accuracy.  \n",
    "- **Business KPIs**: uplift in AOV, revenue, and margin under simulated promos.  \n",
    "- Bias/variance trade-off assessment to avoid overfitting.  \n",
    "\n",
    "### 6. 🚀 Deployment Pipeline\n",
    "- Wrap the model in a **FastAPI service**.  \n",
    "- Deploy with **Kedro pipelines** orchestrated via **GCP (Cloud Run, Cloud Scheduler, GCS Buckets)**.  \n",
    "- Enable **CI/CD** for automated retraining and logging with MLflow or similar tools.  \n",
    "\n",
    "---\n",
    "\n",
    "✅ The quick script shows feasibility,  \n",
    "but the **full-stack ML approach** ensures robustness, interpretability, and scalability to production."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea54fbc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======================================================\n",
    "# 📌 Kedro Pipeline for ML-Driven Bundle AOV Prediction\n",
    "# ======================================================\n",
    "\n",
    "from kedro.pipeline import Pipeline, node\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import joblib\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# ===============================\n",
    "# 1️⃣ ML Training Node\n",
    "# ===============================\n",
    "def train_model(df_pairs: pd.DataFrame) -> GradientBoostingRegressor:\n",
    "    features = [\n",
    "        \"bundle_total_revenue\", \"bundle_total_margin\", \n",
    "        \"bundle_total_quantity\",\n",
    "        \"bundle_conversion_rate\", \"bundle_ctr\", \n",
    "        \"is_fam_1\", \"is_newbie_order_1\"\n",
    "    ]\n",
    "    target = \"bundle_aov\"\n",
    "    \n",
    "    X = df_pairs[features]\n",
    "    y = df_pairs[target]\n",
    "    \n",
    "    model = GradientBoostingRegressor(n_estimators=100, learning_rate=0.01, max_depth=5)\n",
    "    model.fit(X, y)\n",
    "    \n",
    "    # Save locally for deployment\n",
    "    joblib.dump(model, \"bundle_aov_model.pkl\")\n",
    "    \n",
    "    return model\n",
    "\n",
    "# ===============================\n",
    "# 2️⃣ Evaluation Node\n",
    "# ===============================\n",
    "def evaluate_model(model: GradientBoostingRegressor, df_pairs: pd.DataFrame) -> float:\n",
    "    features = [\n",
    "        \"bundle_total_revenue\", \"bundle_total_margin\", \n",
    "        \"bundle_total_quantity\",\n",
    "        \"bundle_conversion_rate\", \"bundle_ctr\",\n",
    "        \"is_fam_1\", \"is_newbie_order_1\"\n",
    "    ]\n",
    "    target = \"bundle_aov\"\n",
    "    \n",
    "    X_val = df_pairs[features]\n",
    "    y_val = df_pairs[target]\n",
    "    y_pred = model.predict(X_val)\n",
    "    \n",
    "    mse = mean_squared_error(y_val, y_pred)\n",
    "    r2 = r2_score(y_val, y_pred)\n",
    "\n",
    "    print(f\"Validation MSE: {mse:.2f}\")\n",
    "    print(f\"Validation R² score: {r2:.4f}\")\n",
    "\n",
    "    return mse, r2\n",
    "\n",
    "# ===============================\n",
    "# 3️⃣ Prediction Node\n",
    "# ===============================\n",
    "def predict_top_bundles(model: GradientBoostingRegressor, top_promo_candidates: pd.DataFrame) -> pd.DataFrame:\n",
    "    features = [\n",
    "        \"bundle_total_revenue\", \"bundle_total_margin\", \n",
    "        \"bundle_total_quantity\",\n",
    "        \"bundle_conversion_rate\", \"bundle_ctr\",\n",
    "        \"is_fam_1\", \"is_newbie_order_1\"\n",
    "    ]\n",
    "    \n",
    "    X_test = top_promo_candidates[features]\n",
    "    top_promo_candidates['predicted_aov'] = model.predict(X_test)\n",
    "    \n",
    "    top_20 = top_promo_candidates.nlargest(20, 'predicted_aov')\n",
    "    return top_20\n",
    "\n",
    "# ===============================\n",
    "# 4️⃣ Kedro Pipeline Definition\n",
    "# ===============================\n",
    "def create_pipeline(**kwargs) -> Pipeline:\n",
    "    return Pipeline(\n",
    "        [\n",
    "            node(\n",
    "                func=train_model,\n",
    "                inputs=\"df_pairs\",\n",
    "                outputs=\"trained_model\",\n",
    "                name=\"train_bundle_model\"\n",
    "            ),\n",
    "            node(\n",
    "                func=evaluate_model,\n",
    "                inputs=[\"trained_model\", \"df_pairs\"],\n",
    "                outputs=\"model_mse\",\n",
    "                name=\"evaluate_bundle_model\"\n",
    "            ),\n",
    "            node(\n",
    "                func=predict_top_bundles,\n",
    "                inputs=[\"trained_model\", \"top_promo_candidates\"],\n",
    "                outputs=\"top_20_predicted_bundles\",\n",
    "                name=\"predict_top_bundles_node\"\n",
    "            )\n",
    "        ]\n",
    "    )\n",
    "\n",
    "df_pairs_sample = df_pairs.sample(n=300000, random_state=42)\n",
    "df_pairs_eval = df_pairs.sample(n=300000, random_state=42)\n",
    "\n",
    "# 1️⃣ Train the model on historical bundles\n",
    "model = train_model(df_pairs_sample)\n",
    "\n",
    "mse, r2 = evaluate_model(model, df_pairs_eval)\n",
    "\n",
    "# 3️⃣ Predict AOV for top promo candidates\n",
    "top_20_bundles = predict_top_bundles(model, top_promo_candidates)\n",
    "\n",
    "print(top_20_bundles[['bundle_name', 'predicted_aov', 'bundle_total_revenue', 'bundle_total_margin']])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "855c88cf",
   "metadata": {},
   "source": [
    "## 🚀 Rollout with ML + GCP Deployment  \n",
    "\n",
    "**Step 1: Train with Kedro Pipelines**  \n",
    "- Define modular nodes for data prep, model training, evaluation, and saving artifacts.  \n",
    "- Run locally or in CI/CD (GitLab CI, GitHub Actions).  \n",
    "\n",
    "**Step 2: Store Artifacts in GCP**  \n",
    "- Save trained models to **Google Cloud Storage (GCS)** bucket:  \n",
    "  `gs://gopuff-model-artifacts/bundle_aov_model.pkl`  \n",
    "\n",
    "**Step 3: Deploy API via Cloud Run**  \n",
    "- Wrap model in a **FastAPI service**.  \n",
    "- Containerize with Docker.  \n",
    "- Deploy container to **Cloud Run**, autoscaling based on traffic.  \n",
    "\n",
    "**Step 4: Automate Retraining with Cloud Scheduler**  \n",
    "- Cloud Scheduler triggers a **Pub/Sub message** to kick off retraining.  \n",
    "- Retraining pipeline runs on **Cloud Run Job** or **Cloud Build**.  \n",
    "- New model replaces old in GCS bucket.  \n",
    "\n",
    "**Step 5: Integration into Gopuff Systems**  \n",
    "- Frontend (app/website) queries Cloud Run API to fetch recommended bundles.  \n",
    "- Backend logs results (CTR, margin, AOV) for monitoring & retraining feedback loop.  \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
